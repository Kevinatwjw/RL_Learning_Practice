{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46fe9379",
   "metadata": {},
   "source": [
    "### æ‚¬å´–æ¼«æ­¥é—®é¢˜ï¼š\n",
    "1. è¦æ±‚ä¸€ä¸ªæ™ºèƒ½ä½“ä»èµ·ç‚¹å‡ºå‘ï¼Œé¿å¼€æ‚¬å´–è¡Œèµ°ï¼Œæœ€ç»ˆåˆ°è¾¾ç›®æ ‡ä½ç½®ã€‚æœ‰ä¸€ä¸ª 4Ã—12 çš„ç½‘æ ¼ä¸–ç•Œï¼Œæ¯ä¸€ä¸ªç½‘æ ¼è¡¨ç¤ºä¸€ä¸ªçŠ¶æ€ã€‚\n",
    "2. æ™ºèƒ½ä½“çš„èµ·ç‚¹æ˜¯å·¦ä¸‹è§’çš„çŠ¶æ€ï¼Œç›®æ ‡æ˜¯å³ä¸‹è§’çš„çŠ¶æ€ï¼Œæ™ºèƒ½ä½“åœ¨æ¯ä¸€ä¸ªçŠ¶æ€éƒ½å¯ä»¥é‡‡å– 4 ç§åŠ¨ä½œï¼šä¸Šã€ä¸‹ã€å·¦ã€å³ã€‚\n",
    "3. å¦‚æœæ™ºèƒ½ä½“é‡‡å–åŠ¨ä½œåè§¦ç¢°åˆ°è¾¹ç•Œå¢™å£åˆ™çŠ¶æ€ä¸å‘ç”Ÿæ”¹å˜ï¼Œå¦åˆ™å°±ä¼šç›¸åº”åˆ°è¾¾ä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚\n",
    "4. ç¯å¢ƒä¸­æœ‰ä¸€æ®µæ‚¬å´–ï¼Œæ™ºèƒ½ä½“æ‰å…¥æ‚¬å´–æˆ–åˆ°è¾¾ç›®æ ‡çŠ¶æ€éƒ½ä¼šç»“æŸåŠ¨ä½œå¹¶å›åˆ°èµ·ç‚¹ï¼Œä¹Ÿå°±æ˜¯è¯´æ‰å…¥æ‚¬å´–æˆ–è€…è¾¾åˆ°ç›®æ ‡çŠ¶æ€æ˜¯ç»ˆæ­¢çŠ¶æ€ã€‚\n",
    "5. æ™ºèƒ½ä½“æ¯èµ°ä¸€æ­¥çš„å¥–åŠ±æ˜¯ âˆ’1ï¼Œæ‰å…¥æ‚¬å´–çš„å¥–åŠ±æ˜¯ âˆ’100ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe8992",
   "metadata": {},
   "source": [
    "#### ReplayBufferç±»ï¼šæ”¯æ’‘off-policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e07e720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import abc # ç”¨äºå®šä¹‰æŠ½è±¡ç±»å’ŒæŠ½è±¡æ–¹æ³•çš„æ¨¡å—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0973f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size = 80):\n",
    "        # å­˜å‚¨çŠ¶æ€å¯¹çš„æ€»é‡\n",
    "        self.max_size = max_size\n",
    "        self.buffer = []\n",
    "    def push_transition(self, transition):\n",
    "        if transition not in self.buffer: # è¿™æ˜¯éå†åšæ³•ï¼Œé€‚åˆå°æ ·æœ¬\n",
    "            self.buffer.append(transition)\n",
    "            if len(self.buffer) > self.max_size:\n",
    "                self.buffer = self.buffer[-self.max_size:] # åªä¿ç•™æœ€æ–°çš„max_sizeä¸ªçŠ¶æ€å¯¹(åªå»æ‰ç¬¬ä¸€ä¸ª)\n",
    "                \n",
    "    def sample(self, batch_size = 5):\n",
    "        # åˆ¤å®šbatch_sizeæ˜¯å¦å¤§äºbufferé•¿åº¦\n",
    "        if batch_size > self.max_size:\n",
    "            raise ValueError(\"é‡‡æ ·çš„é•¿åº¦å¤§äºç»éªŒå›æ”¾æ± å¤§å°ï¼\")\n",
    "        # éšæœºé‡‡æ ·\n",
    "        return random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "    def isfull(self):\n",
    "        \"\"\"æŸäº›å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚ off-policy Q-learningï¼‰ä¼šç­‰ buffer æ»¡äº†æ‰å¼€å§‹æ›´æ–°ï¼›\"\"\"\n",
    "        return len(self.buffer) == self.max_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fa095c",
   "metadata": {},
   "source": [
    "#### å¯¹äºå„ç§ TD Learning æ–¹æ³•è€Œè¨€ï¼Œé™¤äº†æ›´æ–°Qä¼°è®¡çš„å…·ä½“æ“ä½œä¸åŒå¤–ï¼Œå…¶ä»–å‡ ä¹éƒ½ç›¸åŒã€‚æ•…æŠ½è±¡å‡ºåŸºç±»class Solver\n",
    "\n",
    "å¼ºåŒ–å­¦ä¹ ä¸­æ—¶åºå·®åˆ†ï¼ˆTDï¼‰æ§åˆ¶ç®—æ³•çš„æŠ½è±¡åŸºç±»ã€‚\n",
    "\n",
    "æœ¬ç±»æä¾›äº†åŸºäºå€¼å‡½æ•°çš„æ§åˆ¶æ–¹æ³•ï¼ˆå¦‚ **Q-learning**ã€**SARSA**ã€**Expected SARSA**ï¼‰çš„é€šç”¨æ¡†æ¶ç»“æ„ï¼Œ  \n",
    "å°è£…äº† Îµ-greedy ç­–ç•¥ã€Q è¡¨åˆå§‹åŒ–ã€è´ªå©ªç­–ç•¥æ›´æ–°ã€çŠ¶æ€å€¼è®¡ç®—ç­‰é€šç”¨åŠŸèƒ½ã€‚\n",
    "\n",
    "å­ç±»éœ€å®ç° `update_Q_table()` æ–¹æ³•ï¼Œä»¥å®šä¹‰å…·ä½“çš„å€¼å‡½æ•°æ›´æ–°è§„åˆ™ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "##### ğŸ§¾ å‚æ•°è¯´æ˜\n",
    "\n",
    "| å‚æ•°å | ç±»å‹ | è¯´æ˜ |\n",
    "|--------|------|------|\n",
    "| `env` | `gym.Env` | Gym ç¯å¢ƒå®ä¾‹ï¼Œéœ€å…·æœ‰ **ç¦»æ•£çš„ observation å’Œ action ç©ºé—´** |\n",
    "| `alpha` | `float` | å­¦ä¹ ç‡ï¼Œæ§åˆ¶ Q å€¼æ›´æ–°çš„å¹…åº¦ |\n",
    "| `gamma` | `float` | æŠ˜æ‰£å› å­ï¼Œç”¨äºå¯¹æœªæ¥å¥–åŠ±è¿›è¡ŒæŠ˜ç° |\n",
    "| `epsilon` | `float` | Îµ-greedy ç­–ç•¥ä¸­çš„æ¢ç´¢æ¦‚ç‡ |\n",
    "| `seed` | `int or None` | éšæœºç§å­ï¼Œç¡®ä¿æ¢ç´¢ç­–ç•¥å¯å¤ç° |\n",
    "| `replay_buffer_size` | `int` | ç»éªŒå›æ”¾æ± æœ€å¤§å®¹é‡ï¼Œæ”¯æŒ off-policy ç®—æ³• |\n",
    "\n",
    "---\n",
    "\n",
    "##### ğŸ“¦ å±æ€§è¯´æ˜\n",
    "| å±æ€§å | ç±»å‹ | è¯´æ˜ |\n",
    "|--------|------|------|\n",
    "| `Q_table` | `np.ndarray` | çŠ¶æ€-åŠ¨ä½œå€¼å‡½æ•° Q(s, a) |\n",
    "| `V_table` | `np.ndarray` | çŠ¶æ€å€¼å‡½æ•° V(s)ï¼Œä»…ç”¨äºå¯è§†åŒ– |\n",
    "| `greedy_policy` | `List[np.ndarray]` | æ¯ä¸ªçŠ¶æ€ä¸‹çš„æœ€ä¼˜åŠ¨ä½œé›†åˆï¼ˆæ”¯æŒå¤šä¸ªå¹¶åˆ—æœ€ä¼˜ï¼‰ |\n",
    "| `policy_is_updated` | `bool` | ç­–ç•¥æ˜¯å¦å·²æ ¹æ®æœ€æ–° Q è¡¨æ›´æ–° |\n",
    "| `rng` | `np.random.RandomState` | æ§åˆ¶æ¢ç´¢è¡Œä¸ºçš„éšæœºæ•°ç”Ÿæˆå™¨ |\n",
    "| `replay_buffer` | `ReplayBuffer` | ç»éªŒå›æ”¾ç¼“å­˜å™¨ï¼Œç”¨äºæ”¯æŒç»éªŒé‡‡æ · |\n",
    "\n",
    "---\n",
    "##### âš ï¸ æ³¨æ„äº‹é¡¹\n",
    "\n",
    "- æœ¬ç±»ä¸º **æŠ½è±¡åŸºç±»**ï¼ˆä½¿ç”¨ `abc` æ¨¡å—ï¼‰ï¼Œä¸èƒ½ç›´æ¥å®ä¾‹åŒ–ï¼›\n",
    "- å¿…é¡»ç”±å­ç±»å®ç° `update_Q_table()` æ–¹æ³•ï¼›\n",
    "- ä»…æ”¯æŒ **ç¦»æ•£çŠ¶æ€ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´**ï¼›\n",
    "- ç­–ç•¥æå‡ä½¿ç”¨ `greedy_policy`ï¼Œæ”¯æŒå¤šæœ€ä¼˜åŠ¨ä½œéšæœºé€‰æ‹©ï¼›\n",
    "- ä¸ `ReplayBuffer` é…åˆå¯å®ç° off-policy å­¦ä¹ ç»“æ„ï¼ˆå¦‚ DQNã€Q-learningï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d214f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver():\n",
    "    def __init__(self,env:gym.Env, alpha=0.1, gamma=0.9, epsilon=0.1, seed=None, replay_buffer_size=80):\n",
    "        # åˆå§‹åŒ–å‡½æ•°\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        # ç¯å¢ƒä¸­æå–åŠ¨ä½œç©ºé—´å¤§å°å’ŒçŠ¶æ€ç©ºé—´å¤§å°ï¼ˆè¦æ±‚ env.action_space å’Œ env.observation_space å¿…é¡»æ˜¯ gym.spaces.Discrete ç±»å‹ï¼‰\n",
    "        self.n_action = env.action_space.n\n",
    "        self.n_state = env.observation_space.n\n",
    "        # åˆå§‹åŒ–Qå€¼è¡¨,æ¯ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹çš„åˆå§‹å€¼è®¾ä¸º 0ï¼ˆä»£è¡¨â€œå®Œå…¨ä¸äº†è§£ç¯å¢ƒâ€ï¼‰\n",
    "        self.Q_table = np.zeros((self.n_state, self.n_action),dtype = np.float32)\n",
    "        # åˆå§‹åŒ–å½“å‰ç­–ç•¥çš„çŠ¶æ€å€¼\n",
    "        self.V_table = np.zeros((self.n_state),dtype = np.float32)\n",
    "        # åˆå§‹æ—¶é»˜è®¤æ¯ä¸ªåŠ¨ä½œéƒ½æ˜¯æœ€ä¼˜çš„(æ¯ä¸ªçŠ¶æ€å¯¹åº”çš„æœ€ä¼˜åŠ¨ä½œåˆ—è¡¨ï¼Œè€Œè¿™ä¸ªåˆ—è¡¨çš„é•¿åº¦æ˜¯ä¸ç¡®å®šçš„ã€å¯èƒ½ä¸åŒçš„->object)\n",
    "        self.greedy_policy = np.array([np.arange(self.n_action)] * self.n_state,dtype = object)\n",
    "        # æ ‡å¿—å˜é‡ï¼šè¡¨ç¤ºå½“å‰ greedy_policy æ˜¯å¦ä¸æœ€æ–° Q è¡¨åŒ¹é…(å½“ Q å€¼æ›´æ–°åï¼Œåº”å°†å…¶è®¾ä¸º False)\n",
    "        self.policy_is_updated = False\n",
    "        self.rng = np.random.RandomState(1) # æ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹éšæœºæ•°ï¼Œä¸å½±å“å…¨å±€\n",
    "        # è®¾ç½®ç»éªŒå›æ”¾æ± \n",
    "        self.replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "    \n",
    "    def take_action(self, state):\n",
    "        \"\"\"ç”¨äºepsilon-greedyç­–ç•¥é€‰æ‹©åŠ¨ä½œ,è¯¥éƒ¨åˆ†å±äºpolicy improvementçš„èŒƒç•´\"\"\"\n",
    "        # ç¡®ä¿ç­–ç•¥å·²ç»æ›´æ–°\n",
    "        if not self.policy_is_updated:\n",
    "            self.update_policy()\n",
    "        # epsilon-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œ\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.rng.randint(self.n_action) # éšæœºé€‰æ‹©åŠ¨ä½œ(ä» [0, n) ä¸­é€‰ä¸€ä¸ªæ•´æ•°)\n",
    "        else:\n",
    "            return self.rng.choice(self.greedy_policy[state]) # ä»å½“å‰æœ€ä¼˜åŠ¨ä½œä¸­éšæœºé€‰æ‹©ä¸€ä¸ªï¼Œé¼“åŠ±ç­–ç•¥å¤šæ ·æ€§\n",
    "    \n",
    "    def update_policy(self):\n",
    "        \"\"\"æ›´æ–°å½“å‰ç­–ç•¥,ä»Q_tableä¸­æå–æœ€ä¼˜åŠ¨ä½œ\"\"\"\n",
    "        # æ‰¾å‡ºæ‰€æœ‰æœ€å¤§çš„Qå€¼å¯¹åº”çš„åŠ¨ä½œ\n",
    "        # è¿™é‡Œçš„np.where()è¿”å›çš„æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œå…ƒç»„ä¸­åŒ…å«äº†æ‰€æœ‰æœ€å¤§å€¼çš„ç´¢å¼•[0]å°†ndarrayæå–å‡ºæ¥\n",
    "        self.greedy_policy = np.array([np.where(self.Q_table[s] == np.max(self.Q_table[s]))[0] \n",
    "                                        for s in range(self.n_state)], dtype=object)\n",
    "        # ç­–ç•¥æ›´æ–°æ ‡å¿—è®¾ä¸º True\n",
    "        self.policy_is_updated = True\n",
    "    \n",
    "    def update_V_table(self):\n",
    "        \"\"\"æ ¹æ®å½“å‰ Q è¡¨å’Œè´ªå©ªç­–ç•¥è®¡ç®—æ¯ä¸ªçŠ¶æ€çš„çŠ¶æ€å€¼å‡½æ•° V(s)\n",
    "        è‹¥æŸä¸ªçŠ¶æ€æ˜¯æ¥è¿‘ç»ˆç‚¹æˆ–é«˜å¥–åŠ±åŒºåŸŸï¼Œé‚£ä¹ˆå®ƒçš„çŠ¶æ€å€¼å‡½æ•° V(s) ä¼šè¾ƒé«˜ï¼›\n",
    "        è‹¥æŸä¸ªçŠ¶æ€æ˜¯æ¥è¿‘éšœç¢ç‰©æˆ–ä½å¥–åŠ±åŒºåŸŸï¼Œé‚£ä¹ˆå®ƒçš„çŠ¶æ€å€¼å‡½æ•° V(s) ä¼šè¾ƒä½ï¼›\n",
    "        å¦‚æœ V å€¼ä»å·¦åˆ°å³ã€ä»èµ·ç‚¹å‘ç»ˆç‚¹é€æ¸å‡é«˜ï¼Œè¯´æ˜ç­–ç•¥åœ¨å­¦ä¹ ä»èµ·ç‚¹èµ°å‘ç›®æ ‡ï¼›\"\"\"\n",
    "        # åˆ¤æ–­ç­–ç•¥æ˜¯å¦æ›´æ–°\n",
    "        if not self.policy_is_updated:\n",
    "            self.update_policy()\n",
    "        # è®¡ç®—æ¯ä¸ªçŠ¶æ€å¯¹çš„çŠ¶æ€å‡½æ•°(V(s)=max_a Q(s,a)=E_(a~pi(Â·|s))[Q(s,a)])\n",
    "        for s in range(self.n_state):\n",
    "            self.V_table[s] = self.Q_table[s][self.greedy_policy[s][0]]\n",
    "            \n",
    "    @abc.abstractmethod\n",
    "    def update_Q_table(self):\n",
    "        \"\"\"æŠ½è±¡å®ç°\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562e577e",
   "metadata": {},
   "source": [
    "#### ä¸€ã€Sarsaï¼š\n",
    "1. Sarsaä¸ºä¸€ç§on-policyç®—æ³•ï¼Œä½¿ç”¨å½“å‰ç­–ç•¥Ï€æ¥äºç¯å¢ƒäº¤äº’ã€‚åœ¨æ¯è½®äº¤äº’æ—¶å…ˆé‡‡æ ·å¾—åˆ°transition $(s_t,a_t,r_t,s_{t+1},a_{t+1})$ã€‚\n",
    "2. **Bellman Expectation Equation**è¿›è¡Œæ›´æ–°ï¼Œå¹¶å¯¹å½“å‰ç­–ç•¥è¿›è¡Œè¯„ä¼°ï¼š\n",
    "$$\n",
    "Q_\\pi(s_t, a_t) \\leftarrow Q_\\pi(s_t, a_t) + \\alpha \\Bigl( r + \\gamma Q_\\pi(s_{t+1}, a_{t+1}) - Q_\\pi(s_t, a_t) \\Bigr)\n",
    "$$\n",
    "å…¶ä¸­ï¼š\n",
    "- $\\alpha$ï¼šå­¦ä¹ ç‡\n",
    "- $\\gamma$ï¼šæŠ˜æ‰£å› å­\n",
    "- $Q_\\pi(s,a)$ï¼šå½“å‰ç­–ç•¥ä¸‹çš„çŠ¶æ€-åŠ¨ä½œå€¼\n",
    "- $a_{t+1} \\sim \\pi(\\cdot | s_{t+1})$ï¼šä¸‹ä¸€æ­¥åŠ¨ä½œä¾æ—§ä»å½“å‰ç­–ç•¥ä¸­é‡‡æ ·ï¼ˆå¦‚ Îµ-greedyï¼‰\n",
    "3. å†ä½¿ç”¨è´ªå©ªç®—æ³•é€‰å–æŸä¸ªçŠ¶æ€ä¸‹åŠ¨ä½œä»·å€¼æœ€å¤§çš„é‚£ä¸ªåŠ¨ä½œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aea3e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sarsa(Solver):\n",
    "    def __init__(self, env:gym.Env, alpha=0.1, gamma=0.9,epsilon=0.1,seed=None):\n",
    "        # è®© Sarsa è‡ªåŠ¨æ‰§è¡Œå®ƒç»§æ‰¿çš„çˆ¶ç±»,å¯ä»¥è‡ªåŠ¨æ‰§è¡Œçˆ¶ç±» __init__() ä¸­çš„æ‰€æœ‰åˆå§‹åŒ–é€»è¾‘ã€‚\n",
    "        super().__init__(env, alpha, gamma, epsilon, seed)\n",
    "        \n",
    "    def update_Q_table(self, state, action, reward, next_state, next_action):\n",
    "        td_target = reward + self.gamma * self.Q_table[next_state, next_action] # è®¡ç®— TD ç›®æ ‡\n",
    "        td_error = td_target - self.Q_table[state, action]\n",
    "        self.Q_table[state, action] += self.alpha * td_error\n",
    "        self.policy_is_updated = False # æ›´æ–° Q è¡¨åï¼Œç­–ç•¥éœ€è¦æ›´æ–°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f8cbb1",
   "metadata": {},
   "source": [
    "#### äºŒã€Expected Sarsa:\n",
    "1. **Expected SARSA çš„æ›´æ–°å…¬å¼ï¼š**\n",
    "    $$\n",
    "    Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r + \\gamma \\mathbb{E}_{a_{t+1} \\sim \\pi(\\cdot|s_{t+1})} \\left[ Q(s_{t+1}, a_{t+1}) \\right] - Q(s_t, a_t) \\right]\n",
    "    $$\n",
    "    åœ¨ Îµ-greedy ç­–ç•¥ä¸‹ï¼ŒåŠ¨ä½œæ¦‚ç‡ä¸ºï¼šè´ªå©ªåŠ¨ä½œä»¥æ¦‚ç‡ $ 1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}|} $ è¢«é€‰æ‹©ï¼Œå…¶ä»–åŠ¨ä½œä»¥ $ \\frac{\\epsilon}{|\\mathcal{A}|} $ è¢«é€‰æ‹©ã€‚å°†å…¶ä»£å…¥æœŸæœ›é¡¹å¯å¾—ï¼š\n",
    "    $$\n",
    "    \\mathbb{E}_{a' \\sim \\pi} \\left[ Q(s', a') \\right] =\n",
    "    \\epsilon \\cdot \\frac{1}{|\\mathcal{A}|} \\sum_{a'} Q(s', a') +\n",
    "    (1 - \\epsilon) \\cdot \\max_{a'} Q(s', a')\n",
    "    $$\n",
    "    å› æ­¤ï¼ŒTD ç›®æ ‡ä¸ºï¼š\n",
    "    $$\n",
    "    Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r + \\gamma \\left( \\epsilon \\cdot \\frac{1}{|\\mathcal{A}|} \\sum_{a'} Q(s_{t+1}, a') + (1 - \\epsilon) \\cdot \\max_{a'} Q(s_{t+1}, a') \\right) - Q(s_t, a_t) \\right]\n",
    "    $$\n",
    "2. **Expected Sarsa** æ¯æ¬¡ç»™å‡ºçš„ TD target éƒ½æ˜¯æ— åçš„ï¼Œå› æ­¤æ¯ä¸€æ­¥æ›´æ–°çš„ç§»åŠ¨æ–¹å‘éƒ½ä¼šç¡®å®šæ€§åœ°å‡å° TD errorï¼Œè¿™æ ·ä¸‹ä¸€æ­¥æ›´æ–°çš„ç§»åŠ¨è·ç¦»ä¹Ÿä¼šç›¸åº”å‡å°ï¼Œç›´åˆ°æœ€å TD error = 0 æ—¶ï¼Œå†ä¼˜åŒ–çš„ç§»åŠ¨è·ç¦»ä¹Ÿå‡å°åˆ° 0ï¼Œå°±å¥½åƒå®ç°äº†ä¸€ç§è‡ªé€‚åº”å­¦ä¹ ç‡çš„æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ï¼Œæ‰€ä»¥å…¶å­¦ä¹ ç‡å¯ä»¥è®¾ç½®ä¸º1ã€‚\n",
    "3. **SARSA ä¾èµ–è¡Œä¸ºç­–ç•¥ï¼Œä¸èƒ½ Off-policy ä½¿ç”¨**  \n",
    "   SARSA çš„ TD ç›®æ ‡ä½¿ç”¨çš„æ˜¯è¡Œä¸ºç­–ç•¥ $ b $ é‡‡æ ·å¾—åˆ°çš„åŠ¨ä½œ $ a' $ï¼Œå³ï¼š\n",
    "   $$\n",
    "   \\text{TD-target}_{\\text{SARSA}} = r + \\gamma Q_b(s', b(s'))\n",
    "   $$\n",
    "   æ‰€ä»¥æ›´æ–°å¿…é¡»ä¾èµ–ç”Ÿæˆæ•°æ®çš„ç­–ç•¥ï¼Œä¸èƒ½å¤ç”¨æ—§æ•°æ®ã€‚\n",
    "4. **Expected SARSA åªä¾èµ–ç›®æ ‡ç­–ç•¥ï¼Œå¯ Off-policy ä½¿ç”¨**  \n",
    "   Expected SARSA çš„ TD ç›®æ ‡æ˜¯ï¼š\n",
    "   $$\n",
    "   \\text{TD-target}_{\\text{Expected}} = r + \\gamma \\mathbb{E}_{a' \\sim \\pi(s')} [Q(s', a')]\n",
    "   $$\n",
    "   ä¸ä¾èµ–è¡Œä¸ºç­–ç•¥ï¼Œå¯ç›´æ¥ä½¿ç”¨ç»éªŒå›æ”¾ç­‰æ—§æ ·æœ¬å®ç° Off-policy å­¦ä¹ ã€‚\n",
    "5. **Q-learning æ˜¯ Expected SARSA çš„ç‰¹ä¾‹**  \n",
    "   è‹¥ç›®æ ‡ç­–ç•¥$ \\pi $ æ˜¯ greedyï¼ˆè´ªå©ªï¼‰ç­–ç•¥ï¼Œåˆ™ï¼š\n",
    "   $$\n",
    "   \\mathbb{E}_{a' \\sim \\pi(s')} [Q(s', a')] = \\max_{a'} Q(s', a')\n",
    "   $$\n",
    "   å³ Q-learning = Expected SARSA + è´ªå©ªç­–ç•¥ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2874025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedSarsa(Solver):\n",
    "    def __init__(self, env:gym.Env, alpha=0.1, gamma=0.9,epsilon=0.1,seed=None):\n",
    "        # è®© Sarsa è‡ªåŠ¨æ‰§è¡Œå®ƒç»§æ‰¿çš„çˆ¶ç±»,å¯ä»¥è‡ªåŠ¨æ‰§è¡Œçˆ¶ç±» __init__() ä¸­çš„æ‰€æœ‰åˆå§‹åŒ–é€»è¾‘ã€‚\n",
    "        super().__init__(env, alpha, gamma, epsilon, seed)\n",
    "        \n",
    "    def update_Q_table(self, state, action, reward, next_state, batch_size=0):\n",
    "        # batch_size = 0ä¸ºon-policy,å¦åˆ™ä¸ºoff-policy\n",
    "        if batch_size == 0: \n",
    "            Q_Exp = (1-self.epsilon) * self.Q_table[next_state].max() + self.epsilon * self.Q_table[next_state].mean()\n",
    "            td_target = reward + self.gamma * Q_Exp # è®¡ç®— TD ç›®æ ‡\n",
    "            td_error = td_target - self.Q_table[state, action]\n",
    "            self.Q_table[state, action] += self.alpha * td_error\n",
    "        else:\n",
    "            self.replay_buffer.push_transition(transition=(state, action, reward, next_state))\n",
    "            transitions = self.replay_buffer.sample(batch_size)\n",
    "            for s,a,r,s_ in transitions:\n",
    "                Q_Exp = (1-self.epsilon) * self.Q_table[s_].max() + self.epsilon * self.Q_table[s_].mean()\n",
    "                td_target = r + self.gamma * Q_Exp # è®¡ç®— TD ç›®æ ‡\n",
    "                td_error = td_target - self.Q_table[s, a]\n",
    "                self.Q_table[s, a] += self.alpha * td_error\n",
    "        self.policy_is_updated = False # æ›´æ–° Q è¡¨åï¼Œç­–ç•¥éœ€è¦æ›´æ–°"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
