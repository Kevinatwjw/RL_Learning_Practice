{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46fe9379",
   "metadata": {},
   "source": [
    "### 悬崖漫步问题：\n",
    "1. 要求一个智能体从起点出发，避开悬崖行走，最终到达目标位置。有一个 4×12 的网格世界，每一个网格表示一个状态。\n",
    "2. 智能体的起点是左下角的状态，目标是右下角的状态，智能体在每一个状态都可以采取 4 种动作：上、下、左、右。\n",
    "3. 如果智能体采取动作后触碰到边界墙壁则状态不发生改变，否则就会相应到达下一个状态。\n",
    "4. 环境中有一段悬崖，智能体掉入悬崖或到达目标状态都会结束动作并回到起点，也就是说掉入悬崖或者达到目标状态是终止状态。\n",
    "5. 智能体每走一步的奖励是 −1，掉入悬崖的奖励是 −100。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe8992",
   "metadata": {},
   "source": [
    "#### ReplayBuffer类：支撑off-policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e07e720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import abc # 用于定义抽象类和抽象方法的模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0973f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size = 80):\n",
    "        # 存储状态对的总量\n",
    "        self.max_size = max_size\n",
    "        self.buffer = []\n",
    "    def push_transition(self, transition):\n",
    "        if transition not in self.buffer: # 这是遍历做法，适合小样本\n",
    "            self.buffer.append(transition)\n",
    "            if len(self.buffer) > self.max_size:\n",
    "                self.buffer = self.buffer[-self.max_size:] # 只保留最新的max_size个状态对(只去掉第一个)\n",
    "                \n",
    "    def sample(self, batch_size = 5):\n",
    "        # 判定batch_size是否大于buffer长度\n",
    "        if batch_size > self.max_size:\n",
    "            raise ValueError(\"采样的长度大于经验回放池大小！\")\n",
    "        # 随机采样\n",
    "        return random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "    def isfull(self):\n",
    "        \"\"\"某些强化学习算法（如 off-policy Q-learning）会等 buffer 满了才开始更新；\"\"\"\n",
    "        return len(self.buffer) == self.max_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fa095c",
   "metadata": {},
   "source": [
    "#### 对于各种 TD Learning 方法而言，除了更新Q估计的具体操作不同外，其他几乎都相同。故抽象出基类class Solver\n",
    "\n",
    "强化学习中时序差分（TD）控制算法的抽象基类。\n",
    "\n",
    "本类提供了基于值函数的控制方法（如 **Q-learning**、**SARSA**、**Expected SARSA**）的通用框架结构，  \n",
    "封装了 ε-greedy 策略、Q 表初始化、贪婪策略更新、状态值计算等通用功能。\n",
    "\n",
    "子类需实现 `update_Q_table()` 方法，以定义具体的值函数更新规则。\n",
    "\n",
    "---\n",
    "\n",
    "##### 参数说明\n",
    "\n",
    "| 参数名 | 类型 | 说明 |\n",
    "|--------|------|------|\n",
    "| `env` | `gym.Env` | Gym 环境实例，需具有 **离散的 observation 和 action 空间** |\n",
    "| `alpha` | `float` | 学习率，控制 Q 值更新的幅度 |\n",
    "| `gamma` | `float` | 折扣因子，用于对未来奖励进行折现 |\n",
    "| `epsilon` | `float` | ε-greedy 策略中的探索概率 |\n",
    "| `seed` | `int or None` | 随机种子，确保探索策略可复现 |\n",
    "| `replay_buffer_size` | `int` | 经验回放池最大容量，支持 off-policy 算法 |\n",
    "\n",
    "---\n",
    "\n",
    "##### 属性说明\n",
    "| 属性名 | 类型 | 说明 |\n",
    "|--------|------|------|\n",
    "| `Q_table` | `np.ndarray` | 状态-动作值函数 Q(s, a) |\n",
    "| `V_table` | `np.ndarray` | 状态值函数 V(s)，仅用于可视化 |\n",
    "| `greedy_policy` | `List[np.ndarray]` | 每个状态下的最优动作集合（支持多个并列最优） |\n",
    "| `policy_is_updated` | `bool` | 策略是否已根据最新 Q 表更新 |\n",
    "| `rng` | `np.random.RandomState` | 控制探索行为的随机数生成器 |\n",
    "| `replay_buffer` | `ReplayBuffer` | 经验回放缓存器，用于支持经验采样 |\n",
    "\n",
    "---\n",
    "##### 注意事项\n",
    "\n",
    "- 本类为 **抽象基类**（使用 `abc` 模块），不能直接实例化；\n",
    "- 必须由子类实现 `update_Q_table()` 方法；\n",
    "- 仅支持 **离散状态空间和动作空间**；\n",
    "- 策略提升使用 `greedy_policy`，支持多最优动作随机选择；\n",
    "- 与 `ReplayBuffer` 配合可实现 off-policy 学习结构（如 DQN、Q-learning）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d214f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver():\n",
    "    def __init__(self,env:gym.Env, alpha=0.1, gamma=0.9, epsilon=0.1, seed=None, replay_buffer_size=80):\n",
    "        # 初始化函数\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        # 环境中提取动作空间大小和状态空间大小（要求 env.action_space 和 env.observation_space 必须是 gym.spaces.Discrete 类型）\n",
    "        self.n_action = env.action_space.n\n",
    "        self.n_state = env.observation_space.n\n",
    "        # 初始化Q值表,每个状态-动作对的初始值设为 0（代表“完全不了解环境”）\n",
    "        self.Q_table = np.zeros((self.n_state, self.n_action),dtype = np.float32)\n",
    "        # 初始化当前策略的状态值\n",
    "        self.V_table = np.zeros((self.n_state),dtype = np.float32)\n",
    "        # 初始时默认每个动作都是最优的(每个状态对应的最优动作列表，而这个列表的长度是不确定的、可能不同的->object)\n",
    "        self.greedy_policy = np.array([np.arange(self.n_action)] * self.n_state,dtype = object)\n",
    "        # 标志变量：表示当前 greedy_policy 是否与最新 Q 表匹配(当 Q 值更新后，应将其设为 False)\n",
    "        self.policy_is_updated = False\n",
    "        self.rng = np.random.RandomState(1) # 每个智能体独立随机数，不影响全局\n",
    "        # 设置经验回放池\n",
    "        self.replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "    \n",
    "    def take_action(self, state):\n",
    "        \"\"\"用于epsilon-greedy策略选择动作,该部分属于policy improvement的范畴\"\"\"\n",
    "        # 确保策略已经更新\n",
    "        if not self.policy_is_updated:\n",
    "            self.update_policy()\n",
    "        # epsilon-greedy 策略选择动作\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.rng.randint(self.n_action) # 随机选择动作(从 [0, n) 中选一个整数)\n",
    "        else:\n",
    "            return self.rng.choice(self.greedy_policy[state]) # 从当前最优动作中随机选择一个，鼓励策略多样性\n",
    "    \n",
    "    def update_policy(self):\n",
    "        \"\"\"更新当前策略,从Q_table中提取最优动作\"\"\"\n",
    "        # 找出所有最大的Q值对应的动作\n",
    "        # 这里的np.where()返回的是一个元组，元组中包含了所有最大值的索引[0]将ndarray提取出来\n",
    "        self.greedy_policy = np.array([np.where(self.Q_table[s] == np.max(self.Q_table[s]))[0] \n",
    "                                        for s in range(self.n_state)], dtype=object)\n",
    "        # 策略更新标志设为 True\n",
    "        self.policy_is_updated = True\n",
    "    \n",
    "    def update_V_table(self):\n",
    "        \"\"\"根据当前 Q 表和贪婪策略计算每个状态的状态值函数 V(s)\n",
    "        若某个状态是接近终点或高奖励区域，那么它的状态值函数 V(s) 会较高；\n",
    "        若某个状态是接近障碍物或低奖励区域，那么它的状态值函数 V(s) 会较低；\n",
    "        如果 V 值从左到右、从起点向终点逐渐升高，说明策略在学习从起点走向目标；\"\"\"\n",
    "        # 判断策略是否更新\n",
    "        if not self.policy_is_updated:\n",
    "            self.update_policy()\n",
    "        # 计算每个状态对的状态函数(V(s)=max_a Q(s,a)=E_(a~pi(·|s))[Q(s,a)])\n",
    "        for s in range(self.n_state):\n",
    "            self.V_table[s] = self.Q_table[s][self.greedy_policy[s][0]]\n",
    "            \n",
    "    @abc.abstractmethod\n",
    "    def update_Q_table(self):\n",
    "        \"\"\"抽象实现\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562e577e",
   "metadata": {},
   "source": [
    "#### 一、SARSA：\n",
    "1. Sarsa为一种on-policy算法，使用当前策略π来于环境交互。在每轮交互时先采样得到transition $(s_t,a_t,r_t,s_{t+1},a_{t+1})$。\n",
    "2. **Bellman Expectation Equation**进行更新，并对当前策略进行评估：\n",
    "$$\n",
    "Q_\\pi(s_t, a_t) \\leftarrow Q_\\pi(s_t, a_t) + \\alpha \\Bigl( r + \\gamma Q_\\pi(s_{t+1}, a_{t+1}) - Q_\\pi(s_t, a_t) \\Bigr)\n",
    "$$\n",
    "其中：\n",
    "- $\\alpha$：学习率\n",
    "- $\\gamma$：折扣因子\n",
    "- $Q_\\pi(s,a)$：当前策略下的状态-动作值\n",
    "- $a_{t+1} \\sim \\pi(\\cdot | s_{t+1})$：下一步动作依旧从当前策略中采样（如 ε-greedy）\n",
    "3. 再使用贪婪算法选取某个状态下动作价值最大的那个动作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aea3e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sarsa(Solver):\n",
    "    def __init__(self, env:gym.Env, alpha=0.1, gamma=0.9,epsilon=0.1,seed=None):\n",
    "        # 让 Sarsa 自动执行它继承的父类,可以自动执行父类 __init__() 中的所有初始化逻辑。\n",
    "        super().__init__(env, alpha, gamma, epsilon, seed)\n",
    "        \n",
    "    def update_Q_table(self, state, action, reward, next_state, next_action):\n",
    "        td_target = reward + self.gamma * self.Q_table[next_state, next_action] # 计算 TD 目标\n",
    "        td_error = td_target - self.Q_table[state, action]\n",
    "        self.Q_table[state, action] += self.alpha * td_error\n",
    "        self.policy_is_updated = False # 更新 Q 表后，策略需要更新"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f8cbb1",
   "metadata": {},
   "source": [
    "#### 二、Expected SARSA:\n",
    "1. **Expected SARSA 的更新公式：**\n",
    "    $$\n",
    "    Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r + \\gamma \\mathbb{E}_{a_{t+1} \\sim \\pi(\\cdot|s_{t+1})} \\left[ Q(s_{t+1}, a_{t+1}) \\right] - Q(s_t, a_t) \\right]\n",
    "    $$\n",
    "    在 ε-greedy 策略下，动作概率为：贪婪动作以概率 $ 1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}|} $ 被选择，其他动作以 $ \\frac{\\epsilon}{|\\mathcal{A}|} $ 被选择。将其代入期望项可得：\n",
    "    $$\n",
    "    \\mathbb{E}_{a' \\sim \\pi} \\left[ Q(s', a') \\right] =\n",
    "    \\epsilon \\cdot \\frac{1}{|\\mathcal{A}|} \\sum_{a'} Q(s', a') +\n",
    "    (1 - \\epsilon) \\cdot \\max_{a'} Q(s', a')\n",
    "    $$\n",
    "    因此，TD 目标为：\n",
    "    $$\n",
    "    Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r + \\gamma \\left( \\epsilon \\cdot \\frac{1}{|\\mathcal{A}|} \\sum_{a'} Q(s_{t+1}, a') + (1 - \\epsilon) \\cdot \\max_{a'} Q(s_{t+1}, a') \\right) - Q(s_t, a_t) \\right]\n",
    "    $$\n",
    "2. **Expected SARSA** 每次给出的 TD target 都是无偏的，因此每一步更新的移动方向都会确定性地减小 TD error，这样下一步更新的移动距离也会相应减小，直到最后 TD error = 0 时，再优化的移动距离也减小到 0，就好像实现了一种自适应学习率的梯度下降优化，所以其学习率可以设置为1。\n",
    "3. **SARSA 依赖行为策略，不能 Off-policy 使用**  \n",
    "   SARSA 的 TD 目标使用的是行为策略 $ b $ 采样得到的动作 $ a' $，即：\n",
    "   $$\n",
    "   \\text{TD-target}_{\\text{SARSA}} = r + \\gamma Q_b(s', b(s'))\n",
    "   $$\n",
    "   所以更新必须依赖生成数据的策略，不能复用旧数据。\n",
    "4. **Expected SARSA 只依赖目标策略，可 Off-policy 使用**  \n",
    "   Expected SARSA 的 TD 目标是：\n",
    "   $$\n",
    "   \\text{TD-target}_{\\text{Expected}} = r + \\gamma \\mathbb{E}_{a' \\sim \\pi(s')} [Q(s', a')]\n",
    "   $$\n",
    "   不依赖行为策略，可直接使用经验回放等旧样本实现 Off-policy 学习。\n",
    "5. **Q-learning 是 Expected SARSA 的特例**  \n",
    "   若目标策略$ \\pi $ 是 greedy（贪婪）策略，则：\n",
    "   $$\n",
    "   \\mathbb{E}_{a' \\sim \\pi(s')} [Q(s', a')] = \\max_{a'} Q(s', a')\n",
    "   $$\n",
    "   即 Q-learning = Expected SARSA + 贪婪策略。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2874025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedSarsa(Solver):\n",
    "    def __init__(self, env:gym.Env, alpha=0.1, gamma=0.9,epsilon=0.1,seed=None):\n",
    "        # 让 Sarsa 自动执行它继承的父类,可以自动执行父类 __init__() 中的所有初始化逻辑。\n",
    "        super().__init__(env, alpha, gamma, epsilon, seed)\n",
    "        \n",
    "    def update_Q_table(self, state, action, reward, next_state, batch_size=0):\n",
    "        # batch_size = 0为on-policy,否则为off-policy\n",
    "        if batch_size == 0: \n",
    "            Q_Exp = (1-self.epsilon) * self.Q_table[next_state].max() + self.epsilon * self.Q_table[next_state].mean()\n",
    "            td_target = reward + self.gamma * Q_Exp # 计算 TD 目标\n",
    "            td_error = td_target - self.Q_table[state, action]\n",
    "            self.Q_table[state, action] += self.alpha * td_error\n",
    "        else:\n",
    "            self.replay_buffer.push_transition(transition=(state, action, reward, next_state))\n",
    "            transitions = self.replay_buffer.sample(batch_size)\n",
    "            for s,a,r,s_ in transitions:\n",
    "                Q_Exp = (1-self.epsilon) * self.Q_table[s_].max() + self.epsilon * self.Q_table[s_].mean()\n",
    "                td_target = r + self.gamma * Q_Exp # 计算 TD 目标\n",
    "                td_error = td_target - self.Q_table[s, a]\n",
    "                self.Q_table[s, a] += self.alpha * td_error\n",
    "        self.policy_is_updated = False # 更新 Q 表后，策略需要更新"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb4cf8",
   "metadata": {},
   "source": [
    "#### 三、N-step SARSA:\n",
    "\n",
    "1. **N-step SARSA 的更新公式：**  \n",
    "   $$\n",
    "   Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ G_{t:t+n} - Q(s_t, a_t) \\right]\n",
    "   $$\n",
    "   其中多步回报 \\( G_{t:t+n} \\) 定义为：  \n",
    "   $$\n",
    "   G_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1} R_{t+n} + \\gamma^{n} Q(s_{t+n}, a_{t+n})\n",
    "   $$\n",
    "2. 与单步 TD（1-step SARSA）相比，N-step SARSA 利用更多即时奖励，降低了偏差，但增加了估计的方差。\n",
    "3. 当展开步数 $n \\to \\infty$ 时，N-step SARSA 退化为蒙特卡洛（Monte Carlo）方法，使用完整轨迹的累积回报：\n",
    "   $$\n",
    "   G_t = \\sum_{k=t+1}^T \\gamma^{k - t - 1} R_k\n",
    "   $$\n",
    "   这种方法无偏，但估计方差较大，且不依赖当前价值函数的估计。\n",
    "4. N-step SARSA 的回报估计在步数较小时，兼具蒙特卡洛方法的低方差和单步 TD 的低偏差优势。 N-step SARSA 可以与 Expected SARSA 结合，形成 N-step Expected SARSA，进一步降低方差，提高更新的稳定性和效率。\n",
    "5. 标准 N-step SARSA 是 On-policy 方法，依赖行为策略采样数据，因此更新必须跟数据生成策略一致.\n",
    "7. 通过引入重要性采样，N-step SARSA 能扩展为 Off-policy 方法，从非目标策略生成的轨迹中修正偏差，实现目标策略的无偏学习。**\n",
    "**重要性采样的数学定义：**  \n",
    "   在状态 $ s $下动作 $ a $ 的重要性采样权重为：  \n",
    "   $$\n",
    "   w(a|s) = \\frac{\\pi(a|s)}{b(a|s)}\n",
    "   $$\n",
    "   其中 $ \\pi $ 是目标策略，$ b $ 是行为策略。\n",
    "   利用行为策略采样，目标策略期望值可写为：  \n",
    "   $$\n",
    "   \\mathbb{E}_{a \\sim \\pi(\\cdot|s)}[f(a)] = \\sum_a \\pi(a|s) f(a) = \\sum_a b(a|s) w(a|s) f(a) = \\mathbb{E}_{a \\sim b(\\cdot|s)}[w(a|s) f(a)]\n",
    "   $$\n",
    " 多步重要性采样比率定义为轨迹区间 $ t $ 到 $ h $ 重要性采样权重的乘积： \n",
    "    $$\n",
    "    \\rho_{t:h} = \\prod_{k=t}^h \\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}\n",
    "    $$\n",
    "\n",
    "多步 TD 目标的 Off-policy 更新公式为： \n",
    "    $$\n",
    "    Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\rho_{t:h} \\left[ G_{t:h} - Q(S_t, A_t) \\right]\n",
    "    $$\n",
    "    其中  \n",
    "    $$\n",
    "    G_{t:h} = \\sum_{i=t+1}^h \\gamma^{i-t-1} R_i + \\gamma^{h-t} Q(S_h, A_h)\n",
    "    $$\n",
    "\n",
    " **重要性采样的优势:**\n",
    " 保证 Off-policy 学习的无偏性，但其乘积形式可能导致权重方差极大，影响训练稳定。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdde10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nstep_SARSA(Solver):\n",
    "    \"\"\"on-policy\"\"\"\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1, seed=None, nstep=5):\n",
    "        super().__init__(env, alpha, gamma, epsilon, seed)\n",
    "        self.nstep = nstep\n",
    "        self.state_list = []\n",
    "        self.action_list = []\n",
    "        self.reward_list = []\n",
    "    \n",
    "    def update_Q_table(self, state, action, reward, next_state, next_action, done):\n",
    "        # 追加当前状态、动作奖励对，注意不用np.append()，因为其每次都会返回一个新的数组，花销大\n",
    "        self.state_list.append(state)\n",
    "        self.action_list.append(action)\n",
    "        self.reward_list.append(reward)\n",
    "        # 判断是否存储了nstep组数据，N-step SARSA 设计的本质就是基于“完整的n步回报”来更新价值函数。\n",
    "        if len(self.state_list) == self.nstep:\n",
    "            # 计算G_{t:t+n}，采用倒序的方法\n",
    "            G = self.Q_table[next_state,next_action] \n",
    "            for i in reversed(range(self.nstep)):\n",
    "                G = self.gamma * G + self.reward_list[i]       \n",
    "            td_target = G # TD-target\n",
    "            # 提取最初的状态动作对，并剔除最老的一组数据\n",
    "            s_t = self.state_list.pop(0)\n",
    "            a_t = self.action_list.pop(0)\n",
    "            self.reward_list.pop(0)\n",
    "            td_error = td_target - self.Q_table[s_t, a_t]\n",
    "            self.Q_table[s_t, a_t] = self.Q_table[s_t, a_t] + self.alpha * td_error\n",
    "        # 如果当智能体进入悬崖或者到达终点，此时大概率不满足长度为nstep,需要特殊处理\n",
    "        if done:\n",
    "            # 终止状态特殊处理说明：\n",
    "            # 当episode提前终止时（如掉下悬崖或到达终点），缓冲区中可能仍有未处理的(s,a,r)序列。\n",
    "            # 这些序列由于不足n_step长度，无法通过常规n_step更新处理，但依然包含有价值的经验信息。\n",
    "            # 计算终止状态的回报（G）：\n",
    "            # 1. 以终止状态的Q值作为初始回报基准（通常Q(s_terminated,a)=0）\n",
    "            # 2. 反向遍历缓冲区，逐步累积折扣奖励（符合Bellman方程的时间顺序）\n",
    "            # 计算G_{t:t+m}，采用倒序的方法\n",
    "            G = self.Q_table[next_state,next_action] \n",
    "            for i in reversed(range(len(self.state_list))):\n",
    "                G = self.gamma * G + self.reward_list[i]       \n",
    "                td_target = G # TD-target\n",
    "                # 提取最初的状态动作对，并剔除最老的一组数据\n",
    "                s_t = self.state_list.pop(0)\n",
    "                a_t = self.action_list.pop(0)\n",
    "                self.reward_list.pop(0)\n",
    "                td_error = td_target - self.Q_table[s_t, a_t]\n",
    "                self.Q_table[s_t, a_t] = self.Q_table[s_t, a_t] + self.alpha * td_error\n",
    "            # 清空列表，开始新的一轮episode\n",
    "            self.state_list = []\n",
    "            self.action_list = []\n",
    "            self.reward_list = []\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8fc613",
   "metadata": {},
   "source": [
    "##### 补充说明：\n",
    "<a id=\"算法背景\"></a>\n",
    "1. 算法背景\n",
    "N-step SARSA是时序差分(TD)学习的重要变体，它：\n",
    "- 结合了MC方法和单步TD方法的优点\n",
    "- 通过n步引导(boostrapping)平衡偏差和方差\n",
    "- 特别适合处理episode长度不固定的场景\n",
    "\n",
    "<a id=\"模拟场景设定\"></a>\n",
    "2. 模拟场景设定\n",
    "2.1 参数配置表\n",
    "| 参数 | 值 | 说明 |\n",
    "|------|----|------|\n",
    "| `n_step` | 3 | 多步更新的步数 |\n",
    "| `gamma` | 0.9 | 折扣因子 |\n",
    "| `alpha` | 0.1 | 学习率 |\n",
    "| 环境 | CliffWalking | 4x12网格世界 |\n",
    "2.2 初始状态\n",
    "```python\n",
    "经验回放缓冲区\n",
    "state_buffer = [s0, s1]  # 已存储的状态序列\n",
    "action_buffer = [a0, a1]  # 已执行的动作序列\n",
    "reward_buffer = [-1, -1]  # 已获得的奖励序列\n",
    "Q表初始值\n",
    "Q_table = {\n",
    "    (s0,a0): 0,  # 初始Q值\n",
    "    (s1,a1): 0,\n",
    "    (s2,a2): 0,\n",
    "    (s3,a3): 0   # 终止状态\n",
    "}\n",
    "```\n",
    "2.3 最新交互事件\n",
    "Agent在状态`s2`执行动作`a2`后：\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[执行a2] --> B[获得r=-100<br>(掉悬崖)]\n",
    "    B --> C[到达s3(终止)]\n",
    "    C --> D[done=True]\n",
    "```\n",
    "<a id=\"完整处理流程\"></a>\n",
    "3. 完整处理流程\n",
    "3.1 数据准备阶段\n",
    "```python\n",
    "将最新transition加入缓冲区\n",
    "state_buffer.append(s2)    # [s0, s1, s2]\n",
    "action_buffer.append(a2)   # [a0, a1, a2]\n",
    "reward_buffer.append(-100) # [-1, -1, -100]\n",
    "```\n",
    "3.2 终止状态处理\n",
    "```python\n",
    "G = Q_table[s3,a3]  # 终止状态Q值初始化为0\n",
    "```\n",
    "迭代更新过程\n",
    "| 迭代次数 | 计算过程 | 更新操作 | 缓冲区变化 |\n",
    "|----------|---------|----------|------------|\n",
    "| 第一次(i=2) | `G = 0.9*0 + (-100) = -100` | 更新(s0,a0):<br>`Q = 0 + 0.1*(-100-0) = -10` | 移除(s0,a0,-1) |\n",
    "| 第二次(i=1) | `G = 0.9*(-100) + (-1) = -91` | 更新(s1,a1):<br>`Q = 0 + 0.1*(-91-0) = -9.1` | 移除(s1,a1,-1) |\n",
    "| 第三次(i=0) | `G = 0.9*(-91) + (-100) = -181.9` | 更新(s2,a2):<br>`Q = 0 + 0.1*(-181.9-0) = -18.19` | 移除(s2,a2,-100) |\n",
    "\n",
    "3.3 最终结果\n",
    "更新后的Q表\n",
    "| 状态-动作对 | 更新公式 | 新Q值 |\n",
    "|-------------|---------|-------|\n",
    "| (s0,a0) | 0 + 0.1 × (-100) | -10.0 |\n",
    "| (s1,a1) | 0 + 0.1 × (-91) | -9.1 |\n",
    "| (s2,a2) | 0 + 0.1 × (-181.9) | -18.19 |\n",
    "\n",
    "缓冲区状态\n",
    "```python\n",
    "assert len(state_buffer) == 0  # 已完全清空\n",
    "```\n",
    "<a id=\"关键机制解析\"></a>\n",
    "4. 关键机制解析\n",
    "4.1 反向更新原理\n",
    "```mermaid\n",
    "graph RL\n",
    "    A[终止Q=0] -->|γ×0 + r2| B[G=-100]\n",
    "    B -->|更新(s2,a2)| C\n",
    "    C -->|γ×(-100) + r1| D[G=-91]\n",
    "    D -->|更新(s1,a1)| E\n",
    "    E -->|γ×(-91) + r0| F[G=-181.9]\n",
    "    F -->|更新(s0,a0)| G\n",
    "```\n",
    "4.2 数学本质\n",
    "采用修正的n步回报：\n",
    "$$\n",
    "G_{t:t+n} = \\sum_{k=0}^{n-1} \\gamma^k R_{t+k+1} + \\gamma^n Q(S_{t+n}, A_{t+n})\n",
    "$$\n",
    "当遇到终止状态时：\n",
    "$$\n",
    "\\gamma^n Q(S_{t+n}, A_{t+n}) = 0\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
