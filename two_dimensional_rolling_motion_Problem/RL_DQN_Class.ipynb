{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5f8e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce187c",
   "metadata": {},
   "source": [
    "\n",
    "#### 经验回放池："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b9114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    ''' 经验回放池 '''\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)        # 先进先出队列\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):  \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):  \n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*transitions)\n",
    "        return np.array(state), np.array(action), reward, np.array(next_state), done\n",
    "\n",
    "    def size(self): \n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edbac16",
   "metadata": {},
   "source": [
    "#### DQN:\n",
    "1. DQN 算法概述\n",
    "DQN 是一种深度强化学习算法，结合 Q-Learning 和 深度神经网络，用于处理高维状态空间（如图像或连续状态）。由 DeepMind 在 2013 和 2015 年提出，广泛应用于 Atari 游戏等任务。DQN 使用神经网络近似 Q 值函数 $Q(s, a; \\theta)$，通过优化时序差分（TD）误差学习最优策略。\n",
    "\n",
    "2. 数学原理\n",
    "2.1 强化学习与马尔可夫决策过程 (MDP)\n",
    "- **强化学习建模为 MDP $(S, A, P, R, \\gamma)$**：\n",
    "状态 $S$：RollingBall 的 $[x, y, v_x, v_y]$。\n",
    "动作 $A$：25 个离散动作（Hashposotion.py）。\n",
    "转移概率 $P(s' | s, a)$：由 GridWorld.py 物理规则隐式定义。\n",
    "奖励 $R(s, a, s')$：-2.0（每步）、-10.0（撞墙）、+300.0（目标）。\n",
    "折扣因子 $\\gamma$：通常 0.99。\n",
    "\n",
    "- **目标：最大化累积折扣奖励**：\n",
    "$$\n",
    "J(\\pi) = \\mathbb{E}{\\pi} \\left[ \\sum{t=0}^\\infty \\gamma^t R(s_t, a_t, s_{t+1}) \\right]\n",
    "$$\n",
    "平稳分布 $d_\\pi(s)$，满足 $d_\\pi^T P_\\pi = d_\\pi^T$，描述策略 $\\pi$ 下的长期状态访问概率。\n",
    "\n",
    "2.2 Q-Learning 与 Bellman 方程\n",
    "Q 值函数 $Q^\\pi(s, a)$ 表示状态 $s$ 下执行动作 $a$，然后按策略 $\\pi$ 行动的预期累积奖励：\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}{\\pi} \\left[ R(s, a, s') + \\gamma \\sum{a'} \\pi(a'|s') Q^\\pi(s', a') \\right]$$\n",
    "最优 Q 值函数 $Q^*(s, a)$ 满足 Bellman 最优方程（《Chapter 8》, Box 7.5）：\n",
    "$$Q^(s, a) = \\mathbb{E} \\left[ R(s, a, s') + \\gamma \\max_{a'} Q^(s', a') \\right]$$\n",
    "Q-Learning 更新：\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ R(s, a, s') + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]$$\n",
    "RollingBall 的连续状态使 Q 表格不可行，需函数逼近。\n",
    "2.3 DQN 的函数逼近\n",
    "DQN 使用神经网络 $Q(s, a; \\theta)$ 近似 $Q^*(s, a)$。\n",
    "\n",
    "表格法：存储所有 $Q(s, a)$，对连续状态不可行。\n",
    "函数逼近：用参数 $\\theta$ 表示 $Q(s, a; \\theta)$，存储效率高，泛化能力强。\n",
    "\n",
    "RollingBall 配置：\n",
    "\n",
    "输入：4 维状态 $[x, y, v_x, v_y]$。\n",
    "输出：25 维 Q 值向量。\n",
    "网络：全连接网络（2-3 层，128 神经元，ReLU 激活）。\n",
    "\n",
    "DQN 优化 Bellman 误差：\n",
    "$$J(\\theta) = \\mathbb{E} \\left[ \\left( R + \\gamma \\max_{a'} Q(S', a'; \\theta) - Q(S, A; \\theta) \\right)^2 \\right]$$\n",
    "直接优化不稳定，需以下技术。\n",
    "3. DQN 的关键技术及数学推导\n",
    "3.1 经验回放（Experience Replay）\n",
    "问题：经验 $(s, a, r, s', \\text{done})$ 具有时间相关性，违背神经网络训练的 i.i.d. 假设。\n",
    "解决方案：回放池存储经验，随机采样批量数据。强调均匀采样确保状态-动作对 $(S, A)$ 近似均匀分布。\n",
    "数学原理：\n",
    "\n",
    "回放池 $\\mathcal{D}$ 存储 $(s, a, r, s', \\text{done})$。\n",
    "采样 $N$ 个经验，损失为：\n",
    "\n",
    "$$L(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\left[ r_i + \\gamma (1 - \\text{done}i) \\max{a'} Q(s'_i, a'; \\theta) - Q(s_i, a_i; \\theta) \\right]^2$$\n",
    "\n",
    "3.2 目标网络（Target Network）\n",
    "问题：目标值 $r + \\gamma \\max_{a'} Q(s', a'; \\theta)$ 随 $\\theta$ 变化，训练不稳定。\n",
    "解决方案：目标网络 $Q(s, a; \\theta^-)$，参数 $\\theta^-$ 定期复制。《Chapter 8》（Section 8.4.1）描述固定 $\\theta^-$ 简化梯度计算：\n",
    "$$L(\\theta) = \\mathbb{E} \\left[ \\left( r + \\gamma \\max_{a'} Q(s', a'; \\theta^-) - Q(s, a; \\theta) \\right)^2 \\right]$$\n",
    "数学推导：\n",
    "\n",
    "目标值：\n",
    "\n",
    "$$y_i = r_i + \\gamma (1 - \\text{done}i) \\max{a'} Q(s'_i, a'; \\theta^-)$$\n",
    "\n",
    "梯度：\n",
    "\n",
    "$$\\nabla_\\theta L(\\theta) = -\\frac{2}{N} \\sum_{i=1}^N \\left[ y_i - Q(s_i, a_i; \\theta) \\right] \\nabla_\\theta Q(s_i, a_i; \\theta)$$\n",
    "\n",
    "每 $k$ 步（如 100），$\\theta^- \\leftarrow \\theta$。\n",
    "\n",
    "\n",
    "3.3 ε-贪婪策略\n",
    "问题：需平衡探索与利用。\n",
    "解决方案：以概率 $\\epsilon$ 随机选择动作，否则选择 $\\arg\\max_a Q(s, a; \\theta)$。《Chapter 8》（Box 8.1）提到探索策略（如 ε-贪婪）确保平稳分布唯一。\n",
    "数学原理：\n",
    "\n",
    "策略：\n",
    "\n",
    "$$\\pi(a|s) = \\begin{cases}\\frac{\\epsilon}{|A|} + (1 - \\epsilon) \\cdot \\mathbb{I}[a = \\arg\\max_{a'} Q(s, a'; \\theta)] & \\text{if } a = \\arg\\max_{a'} Q(s, a'; \\theta) \\\\frac{\\epsilon}{|A|} & \\text{otherwise}\\end{cases}$$\n",
    "\n",
    "衰减：\n",
    "\n",
    "$$\\epsilon_t = \\epsilon_{\\text{end}} + (\\epsilon_{\\text{start}} - \\epsilon_{\\text{end}}) \\exp(-t / \\tau)$$\n",
    "与 RollingBall 的结合：\n",
    "\n",
    "env.action_space.sample() 实现随机动作。\n",
    "衰减 $\\epsilon$（如 1.0 到 0.1）鼓励目标探索。\n",
    "\n",
    "4. DQN 算法流程\n",
    "综合《Chapter 8》（Algorithm 8.3）和 hrl.boyuai.com：\n",
    "\n",
    "初始化：\n",
    "\n",
    "在线网络 $Q(s, a; \\theta)$，目标网络 $Q(s, a; \\theta^-)$，$\\theta^- \\leftarrow \\theta$。\n",
    "回放池 $\\mathcal{D}$，容量 capacity。\n",
    "超参数：学习率 $\\alpha$，折扣因子 $\\gamma$，$\\epsilon$ 参数。\n",
    "\n",
    "交互：\n",
    "\n",
    "按 ε-贪婪选择 $a$。\n",
    "执行 $a$，获取 $r, s', \\text{done}$。\n",
    "存储 $(s, a, r, s', \\text{done})$。\n",
    "\n",
    "优化：\n",
    "\n",
    "采样批量经验。\n",
    "\n",
    "目标值：\n",
    "$$y_i = r_i + \\gamma (1 - \\text{done}i) \\max{a'} Q(s'_i, a'; \\theta^-)$$\n",
    "\n",
    "损失：\n",
    "$$L(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\left[ y_i - Q(s_i, a_i; \\theta) \\right]^2$$\n",
    "\n",
    "更新：\n",
    "\n",
    "每 $k$ 步，$\\theta^- \\leftarrow \\theta$。\n",
    "衰减 $\\epsilon_t$。\n",
    "\n",
    "5. 局限与改进\n",
    "\n",
    "Q 值过估计：\n",
    "\n",
    "$\\max_{a'} Q(s', a'; \\theta^-)$ 可能高估。《Chapter 10》（Section 10.2）提到基线减少方差，DQN 用 Double DQN：\n",
    "$$y_i = r_i + \\gamma (1 - \\text{done}_i) Q(s'i, \\arg\\max{a'} Q(s'_i, a'; \\theta); \\theta^-)$$\n",
    "\n",
    "稀疏奖励：RollingBall 的 +300.0 奖励稀疏，需奖励整形。\n",
    "\n",
    "离散动作：DQN 限于离散动作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33fc1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Net(torch.nn.Module):\n",
    "    ''' Q 网络是一个两层 MLP, 用于 DQN 和 Double DQN '''\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        # 初始化权重\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"使用Kaiming初始化权重，适合激活函数为Relu\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # 使用 Kaiming 均匀初始化，指定 nonlinearity='relu'\n",
    "                nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')\n",
    "                # 偏置置零\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd96e447",
   "metadata": {},
   "source": [
    "1. self.bn1 = nn.BatchNorm1d(hidden_dim) 是定义一个一维批归一化（Batch Normalization）层的操作。\n",
    "- **核心功能**针对全连接层（Linear）或一维卷积层（Conv1D）的输出.对每个隐藏层神经元的输出进行标准化处理：\n",
    "    $$\n",
    "    \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\quad \\text{(标准化)}\n",
    "    $$\n",
    "    $$\n",
    "    y_i = \\gamma \\hat{x}_i + \\beta \\quad \\text{(可学习缩放和平移)}\n",
    "    $$\n",
    "  其中：\n",
    "$μ_B$是该神经元在当前batch上的均值,$σ_B^2$是方差,γ(weight)和β(bias)是可学习的参数。\n",
    "\n",
    "- **在DQN中的具体作用:**\n",
    "\n",
    "    正向传播时：标准化过程计算当前batch中每个神经元的均值和方差，对128个神经元的输出独立标准化，通过γ和β保留网络的表达能力\n",
    "\n",
    "    反向传播时：自动更新当前batch的$μ_B$和$σ_B^2$、可学习参数γ和β、全局统计量（用于推理的running_mean和running_var）\n",
    "\n",
    "- **数据流维度变化：**\n",
    "\n",
    "    输入: [batch_size, 4] \n",
    "    \n",
    "    fc1: [batch_size, 128] \n",
    "\n",
    "    bn1: [batch_size, 128] (标准化后)\n",
    "\n",
    "    fc2: [batch_size, 128]\n",
    "\n",
    "    输出: [batch_size, 25]\n",
    "    通过这种设计，BatchNorm1d能有效提升DQN在RollingBall环境中的训练稳定性和收敛速度。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6909c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    ''' DQN算法 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, action_range, lr, gamma, epsilon, device, tau=0.001, seed=None):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.action_range = action_range        # action 取值范围\n",
    "        self.gamma = gamma                      # 折扣因子\n",
    "        self.epsilon = epsilon                  # epsilon-greedy\n",
    "        self.tau = tau      # 目标网络更新频率\n",
    "        self.count = 0                          # Q_Net 更新计数\n",
    "        self.rng = np.random.RandomState(seed)  # agent 使用的随机数生成器\n",
    "        self.device = device                \n",
    "        \n",
    "        # Q 网络\n",
    "        self.q_net = Q_Net(state_dim, hidden_dim, action_range).to(device)  \n",
    "        # 目标网络\n",
    "        self.target_q_net = Q_Net(state_dim, hidden_dim, action_range).to(device)\n",
    "        # 使用Adam优化器\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "        \n",
    "    def max_q_value_of_given_state(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float).to(self.device)\n",
    "        return self.q_net(state).max().item()\n",
    "        \n",
    "    def take_action(self, state):  \n",
    "        ''' 按照 epsilon-greedy 策略采样动作 '''\n",
    "        if self.rng.random() < self.epsilon:\n",
    "            action = self.rng.randint(self.action_range)\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float).to(self.device)\n",
    "            action = self.q_net(state).argmax().item()\n",
    "        return action\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'], dtype=torch.float).to(self.device)                             # (bsz, state_dim)\n",
    "        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float).to(self.device)                   # (bsz, state_dim)\n",
    "        actions = torch.tensor(transition_dict['actions'], dtype=torch.int64).view(-1, 1).to(self.device)               # (bsz, act_dim)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device).squeeze()     # (bsz, )\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device).squeeze()         # (bsz, )\n",
    "\n",
    "        q_values = self.q_net(states).gather(dim=1, index=actions).squeeze()                # (bsz, )\n",
    "        max_next_q_values = self.target_q_net(next_states).max(axis=1)[0]                   # (bsz, )\n",
    "        q_targets = rewards + self.gamma * max_next_q_values * (1 - dones)                  # (bsz, )\n",
    "\n",
    "        dqn_loss = torch.mean(F.mse_loss(q_values, q_targets))  \n",
    "        self.optimizer.zero_grad()                                                         \n",
    "        dqn_loss.backward() \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # 软更新目标网络参数\n",
    "        for target_param, q_param in zip(self.target_q_net.parameters(), self.q_net.parameters()):\n",
    "            target_param.data.copy_(self.tau * q_param.data + (1.0 - self.tau) * target_param.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcdf21f",
   "metadata": {},
   "source": [
    "#### DoubleDQN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfbb5434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQN(DQN):\n",
    "    ''' Double DQN算法 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, action_range, lr, gamma, epsilon, device, tau=0.001, seed=None):\n",
    "        super().__init__(state_dim, hidden_dim, action_dim, action_range, lr, gamma, epsilon, device, tau, seed)\n",
    "    \n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'], dtype=torch.float).to(self.device)                             # (bsz, state_dim)\n",
    "        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float).to(self.device)                   # (bsz, state_dim)\n",
    "        actions = torch.tensor(transition_dict['actions'], dtype=torch.int64).view(-1, 1).to(self.device)               # (bsz, act_dim)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device).squeeze()     # (bsz, )\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device).squeeze()         # (bsz, )\n",
    "        # Double DQN：主网络选择动作，目标网络估计Q值\n",
    "        q_values = self.q_net(states).gather(dim=1, index=actions).squeeze()                # (bsz, )\n",
    "        # 使用Q网络估计最优动作（[0]取最优值，[1]取最优值的索引）\n",
    "        max_actions_index = self.q_net(next_states).max(axis=1)[1]\n",
    "        # 由目标网络计算Q值\n",
    "        max_next_q_values = self.target_q_net(next_states).gather(dim=1, index = max_actions_index.unsqueeze(1)).squeeze()                   # (bsz, )\n",
    "        q_targets = rewards + self.gamma * max_next_q_values * (1 - dones)                  # (bsz, )\n",
    "\n",
    "        dqn_loss = torch.mean(F.mse_loss(q_values, q_targets))  \n",
    "        self.optimizer.zero_grad()                                                         \n",
    "        dqn_loss.backward() \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # 软更新目标网络参数\n",
    "        for target_param, q_param in zip(self.target_q_net.parameters(), self.q_net.parameters()):\n",
    "            target_param.data.copy_(self.tau * q_param.data + (1.0 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21abddb4",
   "metadata": {},
   "source": [
    "#### Dueling DQN:\n",
    "\n",
    "Dueling DQN（Dueling Deep Q-Network）是一种改进的深度强化学习算法，通过优化神经网络结构来提升性能，尤其在动作空间较大或状态复杂的情况下。研究表明，Dueling DQN 通过将 Q 值函数分解为状态价值和动作优势两个部分，能够更高效地学习策略，减少 Q 值过估计，并提高训练稳定性。以下是其数学原理和各个方面的简要概述，适合初学者理解。\n",
    "\n",
    "- **关键点**：\n",
    "  - Dueling DQN 将 Q 值分解为状态价值 $ V(s) $ 和动作优势 $ A(s, a) $，提高学习效率。\n",
    "  - 它使用两个神经网络分支：一个估计状态价值，一个估计动作优势。\n",
    "  - 损失函数与标准 DQN 相同，但作用于分解后的 Q 值。\n",
    "  - 相比标准 DQN，Dueling DQN 在动作相似或动作空间大的环境中表现更优。\n",
    "  - 没有显著争议，但其效果依赖于环境特性，可能不总是优于标准 DQN。\n",
    "\n",
    "###### 什么是 Dueling DQN？\n",
    "Dueling DQN 是标准 DQN 的改进版本。标准 DQN 使用神经网络直接预测每个动作的 Q 值（即在状态 $ s $ 下选择动作 $ a $ 的预期回报）。Dueling DQN 则将 Q 值分解为两部分：状态价值（表示状态的好坏）和动作优势（表示动作的相对优劣）。这种分解让模型能更高效地学习状态的重要性，尤其在动作选择影响较小的环境中。\n",
    "\n",
    "###### 为什么更有效？\n",
    "在某些环境中（如 Atari 游戏），许多动作的回报差异不大。Dueling DQN 通过单独学习状态价值，减少对每个动作的逐一评估，从而更快地学习到好的策略。它还通过减去平均优势值来稳定训练，减少 Q 值过估计的问题。\n",
    "\n",
    "###### 如何实现？\n",
    "Dueling DQN 使用一个共享的特征提取层（例如全连接层或卷积层），然后分为两个分支：一个输出状态价值 $ V(s) $，另一个输出动作优势 $ A(s, a) $。最终 Q 值通过公式组合：\n",
    "$$\n",
    "Q(s, a) = V(s) + \\left( A(s, a) - \\frac{1}{|\\mathcal{A}|} \\sum_{a'} A(s, a') \\right)\n",
    "$$\n",
    "训练过程与标准 DQN 类似，使用经验回放和目标网络来优化损失函数。\n",
    "\n",
    "###### 适用场景\n",
    "Dueling DQN 特别适合动作空间较大或状态复杂的任务，例如你的 `RollingBall` 环境（100 个离散动作）。它能更高效地处理动作选择不敏感的状态，提高学习速度。\n",
    "\n",
    "---\n",
    "\n",
    "##### Dueling DQN 的数学原理\n",
    "\n",
    "1. 背景与动机\n",
    "\n",
    "**标准 DQN 的局限性**：\n",
    "- **直接估计 Q 值**：标准 DQN 使用神经网络直接估计 $ Q(s, a) $，输出每个动作的 Q 值。这在动作空间较大时计算量大。\n",
    "- **动作无关性**：在某些环境中（如 Atari 游戏），许多动作对长期回报的影响相似，DQN 无法有效利用这一特性，导致学习效率低下。\n",
    "- **Q 值过估计**：由于最大化操作（$\\max_a Q(s', a')$），DQN 可能高估 Q 值，影响策略稳定性。\n",
    "\n",
    "**Dueling DQN 的创新**：\n",
    "- Dueling DQN 将 Q 值函数分解为状态价值 $ V(s) $ 和动作优势 $ A(s, a) $，通过两个神经网络分支分别学习：\n",
    "  - $ V(s) $：表示状态 $ s $ 的整体价值，与动作无关。\n",
    "  - $ A(s, a) $：表示在状态 $ s $ 下选择动作 $ a $ 相对于平均动作价值的优势。\n",
    "- Q 值公式：\n",
    "  $$\n",
    "  Q(s, a) = V(s) + A(s, a)\n",
    "  $$\n",
    "- 为确保可识别性（避免 $ V(s) $ 和 $ A(s, a) $ 的分解不唯一），通常减去平均优势值：\n",
    "  $$\n",
    "  Q(s, a; \\theta, \\alpha, \\beta) = V(s; \\theta, \\beta) + \\left( A(s, a; \\theta, \\alpha) - \\frac{1}{|\\mathcal{A}|} \\sum_{a'} A(s, a'; \\theta, \\alpha) \\right)\n",
    "  $$\n",
    "  - $ \\theta $：共享特征提取层的参数。\n",
    "  - $ \\beta $：价值流的参数。\n",
    "  - $ \\alpha $：优势流的参数。\n",
    "  - $ |\\mathcal{A}| $：动作空间大小。\n",
    "\n",
    "**动机**：\n",
    "- 分离状态价值和动作优势可以更高效地学习状态的重要性，尤其在动作选择对回报影响较小的环境中。\n",
    "- 减去平均优势值提高训练稳定性，确保优势函数的平均值为零。\n",
    "\n",
    "2. 网络结构\n",
    "\n",
    "**Dueling DQN 的网络结构**：\n",
    "- **共享特征提取层**：通常是卷积层（对于图像输入，如 Atari 游戏）或全连接层（对于向量输入，如 `RollingBall` 环境）。这些层提取状态的通用特征。\n",
    "- **价值流**：输出单一标量 $ V(s) $，表示状态价值。\n",
    "- **优势流**：输出与动作空间同维的向量 $ A(s, a) $，表示每个动作的优势。\n",
    "- **组合层**：将价值流和优势流的结果组合，计算 Q 值。\n",
    "\n",
    "**数学表示**：\n",
    "- 输入：状态 $ s $（例如，4 维向量 `[x_position, y_position, x_velocity, y_velocity]`）。\n",
    "- 共享特征提取：$ f(s; \\theta) $，生成特征向量。\n",
    "- 价值流：$ V(s; \\theta, \\beta) = W_v f(s; \\theta) + b_v $，输出标量。\n",
    "- 优势流：$ A(s, a; \\theta, \\alpha) = W_a f(s; \\theta) + b_a $，输出向量。\n",
    "- 最终 Q 值：\n",
    "  $$\n",
    "  Q(s, a) = V(s) + \\left( A(s, a) - \\frac{1}{|\\mathcal{A}|} \\sum_{a'} A(s, a') \\right)\n",
    "  $$\n",
    "\n",
    "3. 损失函数\n",
    "\n",
    "**损失函数**:\n",
    "- Dueling DQN 使用与标准 DQN 相同的基于时间差分（TD）误差的损失函数：\n",
    "  $$\n",
    "  theta, \\alpha, \\beta) = \\mathbb{E}_{(s, a, r, s', d) \\sim D} \\left[ \\left( r + \\gamma (1-d) \\max_{a'} Q(s', a'; \\theta^-, \\alpha^-, \\beta^-) - Q(s, a; \\theta, \\alpha, \\beta) \\right)^2 \\right]\n",
    "  $$\n",
    "  - $ r $：即时奖励。\n",
    "  - $ \\gamma $：折扣因子（例如，代码中为 0.99）。\n",
    "  - $ d $：终止标志（done）。\n",
    "  - $ D $：经验回放缓冲区。\n",
    "  - $ \\theta^-, \\alpha^-, \\beta^- $：目标网络参数。\n",
    "- 目标 Q 值由目标网络计算，目标网络定期更新以稳定训练：\n",
    "  $$\n",
    "  \\theta^- \\leftarrow \\tau \\theta + (1 - \\tau) \\theta^-\n",
    "  $$\n",
    "  - $ \\tau $：软更新参数（例如，代码中为 0.001）。\n",
    "\n",
    "**与标准 DQN 的区别**：\n",
    "- 损失函数作用于分解后的 Q 值（通过价值流和优势流计算）。\n",
    "- 分解结构有助于减少 Q 值过估计，因为状态价值 $ V(s) $ 提供了一个稳定的基线。\n",
    "\n",
    "#### 4. 与标准 DQN 的区别\n",
    "\n",
    "| **方面**            | **标准 DQN**                              | **Dueling DQN**                              |\n",
    "|---------------------|------------------------------------------|---------------------------------------------|\n",
    "| **Q 值估计**        | 直接估计 $ Q(s, a) $                 | 分解为 $ V(s) $ 和 $ A(s, a) $          |\n",
    "| **网络输出**        | 每个动作的 Q 值                         | 状态价值 $ V(s) v 和动作优势 $ A(s, a) $ |\n",
    "| **泛化性**          | 较差，尤其在动作相似时                 | 更好，独立学习状态价值                      |\n",
    "| **稳定性**          | 可能出现 Q 值过估计                    | 通过分解减少过估计                          |\n",
    "| **计算效率**        | 在大动作空间中效率较低                 | 通过分解提高效率                            |\n",
    "\n",
    "**具体差异**：\n",
    "- **动作无关性**：Dueling DQN 能更好地处理动作选择对回报影响较小的状态，通过学习 $ V(s) $ 减少冗余计算。\n",
    "- **过估计缓解**：分解结构使 Q 值估计更稳定，尤其在结合 Double DQN 时。\n",
    "- **效率提升**：在动作空间较大时（如 100 个动作），Dueling DQN 通过单一状态价值和相对优势值减少计算负担。\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b88d684",
   "metadata": {},
   "source": [
    "#### 同时拟合状态价值V(s)和动作优势A(s,a)，并通过组合公式计算Q值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df8e6ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VA_net(torch.nn.Module):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        torch (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(VA_net, self).__init__()\n",
    "        # 共享网络部分\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2= torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        # 输出每个动作的优势值A(s,a)，维度为动作空间的大小\n",
    "        self.fc_A = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        # 输出状态价值V(s),维度为1\n",
    "        self.fc_V = torch.nn.Linear(hidden_dim, 1)\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"使用Kaiming初始化权重，适合激活函数为Relu\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # 使用 Kaiming 均匀初始化，指定 nonlinearity='relu'\n",
    "                nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')\n",
    "                # 偏置置零\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = F.relu(self.fc2(x))\n",
    "        A = self.fc_A(F.relu(x))\n",
    "        V = self.fc_V(F.relu(x))\n",
    "        Q = V + A - A.mean().item()\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377171f2",
   "metadata": {},
   "source": [
    "#### Dueling DQN："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a7e04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDON(DQN):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, action_range, lr, gamma, epsilon, device, tau=0.001, seed=None):\n",
    "        super().__init__(state_dim, hidden_dim, action_dim, action_range, lr, gamma, epsilon, device, tau, seed)\n",
    "        \n",
    "        self.q_net = VA_net(state_dim, hidden_dim, action_range).to(self.device)\n",
    "        self.target_q_net = VA_net(state_dim, hidden_dim, action_range).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr = lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ef1baa",
   "metadata": {},
   "source": [
    "#### Reinforce & Actor-Critic:\n",
    "\n",
    "**什么是 REINFORCE？**  \n",
    "REINFORCE 是一种策略梯度方法，通过直接调整策略参数来最大化累积奖励。它使用蒙特卡洛方法，收集完整的情节（状态-动作-奖励序列），然后根据回报计算梯度更新策略。适合处理大型状态空间，但因梯度方差高，可能学习较慢。  \n",
    "\n",
    "**什么是 Actor-Critic？**  \n",
    "Actor-Critic 结合策略（Actor）和价值函数（Critic），Actor 选择动作，Critic 评估动作质量。使用时序差分学习，样本效率更高，适合复杂环境如机器人控制。包括多种变体，如 A2C 和 DDPG，特别在连续动作空间中表现优异。  \n",
    "\n",
    "**两者的对比**  \n",
    "- **效率：** Actor-Critic 通常比 REINFORCE 更高效，因其使用 Critic 降低梯度方差。  \n",
    "- **适用场景：** REINFORCE 适合简单任务，Actor-Critic 更适合复杂、连续动作的任务。   \n",
    "\n",
    "---\n",
    "\n",
    "#### REINFORCE 算法详解  \n",
    "REINFORCE（也被称为Monte Carlo Policy Gradient）是一种通过调整策略参数 $\\theta$ 来优化策略 $\\pi_\\theta(a \\mid s)$ 的方法。在强化学习中，策略 $\\pi_\\theta(a \\mid s)$ 定义了在状态 $s$ 下选择动作 $a$ 的概率。其目标是最大化期望累积奖励 $J(\\theta)$，数学上表示为：\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} [R(\\tau)]\n",
    "$$\n",
    "\n",
    "- $\\tau = (s_0, a_0, s_1, a_1, \\dots, s_T)$ 是一个轨迹（trajectory），由状态和动作序列组成。\n",
    "- $R(\\tau) = \\sum_{t=0}^T \\gamma^t r_t$ 是轨迹的总回报，其中 $\\gamma \\in [0, 1]$ 是折扣因子，$r_t$ 是每一步的奖励。\n",
    "\n",
    "---\n",
    "\n",
    "2. **策略梯度定理**\n",
    "REINFORCE的核心是**策略梯度定理（Policy Gradient Theorem）**，它提供了目标函数 $J(\\theta)$ 关于策略参数 $\\theta$ 的梯度表达式：\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t) \\cdot G_t \\right]\n",
    "$$\n",
    "\n",
    "- $G_t = \\sum_{k=t}^T \\gamma^{k-t} r_k$ 是从时间步 $t$ 到轨迹结束的总回报。\n",
    "- $\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)$ 是策略对动作 $a_t$ 在状态 $s_t$ 下概率的对数梯度。\n",
    "\n",
    "这个定理告诉我们，沿着梯度方向调整 $\\theta$ 可以增加期望回报。\n",
    "\n",
    "---\n",
    "\n",
    "3. **蒙特卡洛估计**\n",
    "由于上述期望无法解析计算，REINFORCE使用**蒙特卡洛采样（Monte Carlo Sampling）**来估计梯度：\n",
    "- 代理（agent）根据当前策略 $\\pi_\\theta$ 与环境交互，生成多个轨迹。\n",
    "- 对于每个轨迹，计算每个时间步的回报 $G_t$。\n",
    "- 使用样本估计梯度：\n",
    "\n",
    "$$\n",
    "\\hat{\\nabla}_\\theta J(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^{T_i} \\nabla_\\theta \\log \\pi_\\theta(a_{i,t} \\mid s_{i,t}) \\cdot G_{i,t}\n",
    "$$\n",
    "\n",
    "- $N$ 是采样的轨迹数量。\n",
    "- 然后通过梯度上升更新策略参数：\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\hat{\\nabla}_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "其中 $\\alpha$ 是学习率。\n",
    "\n",
    "---\n",
    "\n",
    "4. **降低方差的基线**\n",
    "REINFORCE的一个主要问题是梯度估计的**高方差**，因为回报 $G_t$ 受环境随机性影响较大。为了减少方差，可以引入一个**基线（Baseline）**，通常是状态价值函数 $V(s_t)$，改进梯度估计为：\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E} \\left[ \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t) \\cdot (G_t - V(s_t)) \\right]\n",
    "$$\n",
    "\n",
    "基线不会改变梯度的无偏性，但通过使回报居中（centering），显著降低方差，从而加速和稳定学习。\n",
    "\n",
    "---\n",
    "\n",
    "5. **算法流程**\n",
    "REINFORCE的工作流程如下：\n",
    "- **初始化**策略参数 $\\theta$。\n",
    "- 使用当前策略 $\\pi_\\theta$ 生成多个完整轨迹。\n",
    "- 对于每个轨迹，计算每个时间步的回报 $G_t$。\n",
    "- 使用样本估计策略梯度 $\\hat{\\nabla}_\\theta J(\\theta)v$。\n",
    "- 更新参数：$\\theta \\leftarrow \\theta + \\alpha \\hat{\\nabla}_\\theta J(\\theta)$。\n",
    "- 重复步骤2-5，直到策略收敛。\n",
    "\n",
    "---\n",
    "\n",
    "6. **数学特性**\n",
    "- **无偏估计**：蒙特卡洛方法提供的梯度估计是无偏的。\n",
    "- **高方差**：依赖完整轨迹的回报导致方差较大，收敛可能较慢。\n",
    "- **收敛性**：在适当的学习率条件下（例如 $\\alpha \\to 0$，$\\sum \\alpha = \\infty$，$\\sum \\alpha^2 < \\infty$），REINFORCE能收敛到局部最优。\n",
    "\n",
    "---\n",
    "\n",
    "7. **优点与局限性**\n",
    "\n",
    "**优点**：\n",
    "- **无模型（Model-Free）**：不需要环境的动态模型。\n",
    "- **支持随机策略**：通过概率性动作选择实现天然的探索。\n",
    "- **函数逼近**：可结合神经网络处理大状态空间。\n",
    "\n",
    "**局限性**：\n",
    "- **高方差**：梯度估计不稳定，学习效率低。\n",
    "- **样本效率低**：需要完整轨迹，适合短时域任务，长时域任务表现不佳。\n",
    "- **在线策略（On-Policy）**：每次更新需要重新采样，计算成本高。\n",
    "\n",
    "---  \n",
    "\n",
    "#### Actor-Critic 算法详解  \n",
    "Actor-Critic 方法结合策略梯度和价值函数，首次由 Barto, Sutton 和 Anderson 在 1983 年提出，是 RL 中的混合方法。它由两个主要组件组成：  \n",
    "- **Actor：** 负责根据当前策略 $ \\pi(a \\mid s, \\theta) $ 选择动作，更新策略以最大化期望回报。  \n",
    "- **Critic：** 评估 Actor 的动作质量，估计价值函数（如状态价值 $ v(s) $ 或动作价值 $ q(s, a) $），通常使用时序差分（TD）学习。  \n",
    "\n",
    "- **核心原理：**  \n",
    "  - Actor-Critic 通过 Critic 的反馈降低策略梯度的方差，相比 REINFORCE 更高效。  \n",
    "  - Critic 的价值估计（如 TD 误差）作为 Actor 更新时的强化信号，公式为：  \n",
    "    $$\n",
    "    \\delta_t = r_t + \\gamma v(s_{t+1}) - v(s_t)\n",
    "    $$\n",
    "    其中 $ \\delta_t $ 是 TD 误差，指导 Actor 更新。  \n",
    "\n",
    "- **主要变体：**  \n",
    "  - **Vanilla Actor-Critic：** 最简单形式，使用 Critic 的 TD 误差直接更新 Actor。  \n",
    "  - **Advantage Actor-Critic (A2C)：** 引入优势函数 $ A(s, a) = q(s, a) - v(s) $，降低方差，更新公式为：  \n",
    "    $$\n",
    "    \\nabla_\\theta J(\\theta) = \\mathbb{E} \\left[ \\nabla_\\theta \\log \\pi(a_t \\mid s_t, \\theta) \\cdot A(s_t, a_t) \\right]\n",
    "    $$\n",
    "    A2C 同时更新 Actor 和 Critic，适合探索性强的任务。  \n",
    "  - **Asynchronous Advantage Actor-Critic (A3C)：** 使用多代理并行学习，提升稳定性和样本效率。  \n",
    "  - **Deep Deterministic Policy Gradient (DDPG)：** 针对连续动作空间的离策略方法，结合 DQN 和 Actor-Critic。  \n",
    "  - **Soft Actor-Critic (SAC)：** 最大化期望回报和策略熵，鼓励探索，适合高维连续任务。  \n",
    "\n",
    "- **优势：**  \n",
    "  - 样本效率高，因使用 TD 学习无需完整情节。  \n",
    "  - 结合策略和价值方法，适合复杂环境如机器人控制和游戏 AI。  \n",
    "  - 多种变体适应不同场景，特别在连续动作空间中表现优异。  \n",
    "\n",
    "- **局限：**  \n",
    "  - 实现复杂度较高，需要同时训练 Actor 和 Critic，调参难度大。  \n",
    "  - Critic 的价值估计可能不准确，影响 Actor 的更新。  \n",
    "\n",
    "Actor-Critic 是政策梯度和价值方法的优雅结合，解决了 REINFORCE 的高方差问题。  \n",
    "\n",
    "#### REINFORCE 与 Actor-Critic 的对比  \n",
    "\n",
    "以下表格总结两者的关键差异：  \n",
    "\n",
    "| **特性**               | **REINFORCE**                          | **Actor-Critic**                       |\n",
    "|-----------------------|---------------------------------------|---------------------------------------|\n",
    "| **估计方法**           | 蒙特卡洛方法，需完整情节              | 时序差分学习，可用部分情节             |\n",
    "| **梯度方差**           | 高，需基线降低                        | 低，Critic 提供基线或优势函数          |\n",
    "| **样本效率**           | 较低，依赖完整序列                    | 较高，适合长序列任务                  |\n",
    "| **学习速度**           | 较慢，因高方差可能不稳定              | 较快，Critic 反馈加速收敛              |\n",
    "| **适用场景**           | 简单任务，离散动作空间                | 复杂任务，连续动作空间（如机器人控制） |\n",
    "| **实现复杂度**         | 较低，单策略更新                      | 较高，需同时更新 Actor 和 Critic       |\n",
    "\n",
    "- **效率对比：** Actor-Critic 因 Critic 的价值估计降低方差，通常比 REINFORCE 更高效，尤其在长序列或高维任务中。  \n",
    "- **适用场景：** REINFORCE 适合初学者理解策略梯度，Actor-Critic 更适合现代 RL 应用，如深度强化学习中的游戏 AI 和自动驾驶。  \n",
    "- **争议：** 一些研究（如 [Actor-critic methods — Mastering Reinforcement Learning](https://gibberblot.github.io/rl-notes/single-agent/actor-critic.html)）指出，Actor-Critic 的优势依赖实现细节，REINFORCE 在某些低维离散任务中可能更稳定。  \n",
    "\n",
    "#### 应用与未来方向  \n",
    "- **REINFORCE：** 常用于教学和简单任务，如迷宫导航，但因样本效率低，在现代 RL 中使用较少。  \n",
    "- **Actor-Critic：** 广泛应用于深度 RL，如 AlphaGo 的策略网络训练，特别在连续动作空间中表现优异，如机器人控制和游戏 AI。  \n",
    "- 未来方向包括改进 Actor-Critic 的样本效率（如离策略方法）和扩展到多代理 RL 系统。  \n",
    "\n",
    "#### 结论  \n",
    "REINFORCE 和 Actor-Critic 各有优势，REINFORCE 适合简单任务，Actor-Critic 更适合复杂环境。选择哪种算法需根据任务需求、动作空间和计算资源权衡。  \n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a39ca28",
   "metadata": {},
   "source": [
    "#### REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c0739",
   "metadata": {},
   "source": [
    "##### 定义策略网络，用简单的MLP："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a29a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    策略网络，用于强化学习中的策略梯度方法（如 REINFORCE）。\n",
    "    网络结构为三层 MLP（多层感知机），输入状态，输出动作概率分布。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        初始化策略网络。\n",
    "\n",
    "        参数：\n",
    "        - input_dim (int): 输入维度，即状态空间的维度。\n",
    "        - hidden_dim (int): 隐藏层维度，控制网络容量。\n",
    "        - output_dim (int): 输出维度，即动作空间的大小（离散动作）。\n",
    "\n",
    "        网络结构：\n",
    "        - fc1: 输入层 -> 隐藏层 1\n",
    "        - fc2: 隐藏层 1 -> 隐藏层 2\n",
    "        - fc3: 隐藏层 2 -> 输出层\n",
    "        \"\"\"\n",
    "        super(PolicyNet, self).__init__()  # 调用父类构造函数，初始化 torch.nn.Module\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)  # 第一层线性变换：输入层 -> 隐藏层 1\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)  # 第二层线性变换：隐藏层 1 -> 隐藏层 2\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, output_dim)  # 第三层线性变换：隐藏层 2 -> 输出层（动作概率）\n",
    "        self._init_weights()  # 调用权重初始化方法，设置初始参数\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        自定义权重初始化方法，确保网络初始行为适合强化学习任务。\n",
    "\n",
    "        目标：\n",
    "        - 隐藏层：使用 Kaiming 初始化，适合 ReLU 激活函数，保持激活值和梯度方差稳定。\n",
    "        - 输出层：使用小方差正态分布，确保初始动作概率分布接近均匀，促进探索。\n",
    "        \"\"\"\n",
    "        # 隐藏层 1 (fc1) 权重初始化\n",
    "        # 使用 Kaiming 初始化（正态分布形式），适合 ReLU 激活函数\n",
    "        # mode='fan_in'：方差计算基于输入神经元数量，避免激活值方差过大或过小\n",
    "        # nonlinearity='relu'：指定激活函数为 ReLU，调整方差为 2/fan_in\n",
    "        torch.nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        \n",
    "        # 隐藏层 1 (fc1) 偏置初始化\n",
    "        # 初始化为 0.1（小正值），确保 ReLU 激活后更多神经元活跃（输出非 0）\n",
    "        # 避免“神经元死亡”（ReLU 输出恒为 0），提升网络初始表达能力\n",
    "        torch.nn.init.constant_(self.fc1.bias, 0)\n",
    "        \n",
    "        # 隐藏层 2 (fc2) 权重初始化\n",
    "        # 同 fc1，使用 Kaiming 初始化，保持深层网络中激活值和梯度的稳定性\n",
    "        torch.nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in', nonlinearity='relu')\n",
    "        \n",
    "        # 隐藏层 2 (fc2) 偏置初始化\n",
    "        # 同 fc1，设置为 0.1，增加神经元活跃性\n",
    "        torch.nn.init.constant_(self.fc2.bias, 0)\n",
    "        \n",
    "        # 输出层 (fc3) 权重初始化\n",
    "        # 使用小方差正态分布（均值 0，标准差 0.01），使初始权重接近 0\n",
    "        # 这样，fc3 层的输出 z = Wx + b 接近 0，softmax(z) 接近均匀分布（1/output_dim）\n",
    "        # 适合强化学习（如 REINFORCE）中初始探索需求，避免过早收敛到次优策略\n",
    "        torch.nn.init.normal_(self.fc3.weight, mean=0, std=0.01)\n",
    "        \n",
    "        # 输出层 (fc3) 偏置初始化\n",
    "        # 设置为 0，确保初始动作概率分布完全由权重决定，避免引入额外偏移\n",
    "        torch.nn.init.zeros_(self.fc3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播，计算状态对应的动作概率分布。\n",
    "\n",
    "        参数：\n",
    "        - x (torch.Tensor): 输入张量，形状为 (batch_size, input_dim)，表示一批状态。\n",
    "\n",
    "        返回：\n",
    "        - torch.Tensor: 动作概率分布，形状为 (batch_size, output_dim)，表示每个动作的概率。\n",
    "        \"\"\"\n",
    "        # 第一层：线性变换 + ReLU 激活\n",
    "        # 输入 x 的形状为 (batch_size, input_dim)\n",
    "        # 输出形状为 (batch_size, hidden_dim)\n",
    "        # ReLU(z) = max(0, z)，激活函数增加非线性，截断负值\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # 第二层：线性变换 + ReLU 激活\n",
    "        # 输入形状为 (batch_size, hidden_dim)\n",
    "        # 输出形状为 (batch_size, hidden_dim)\n",
    "        # 进一步增加网络深度和非线性表达能力\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # 第三层：线性变换 + Softmax 激活\n",
    "        # 输入形状为 (batch_size, hidden_dim)\n",
    "        # 输出 z 的形状为 (batch_size, output_dim)，表示每个动作的未归一化得分（logits）\n",
    "        # Softmax 将 logits 转换为概率分布，确保 sum(probs) = 1\n",
    "        # 数学上：softmax(z)_i = exp(z_i) / sum(exp(z_j))\n",
    "        # dim=1 表示在动作维度上归一化\n",
    "        return F.softmax(self.fc3(x), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51b8fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    REINFORCE 算法实现，基于策略梯度方法，用于强化学习任务。\n",
    "    该类包含策略网络（PolicyNet）和优化器，通过蒙特卡洛方法估计梯度并更新策略参数。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, hidden_dim, action_range, learning_rate, gamma, device):\n",
    "        \"\"\"\n",
    "        初始化 REINFORCE 算法。\n",
    "\n",
    "        参数：\n",
    "        - state_dim (int): 状态空间的维度（输入维度）。\n",
    "        - hidden_dim (int): 策略网络隐藏层的维度，控制网络容量。\n",
    "        - action_range (int): 动作空间的大小（离散动作数量，即输出维度）。\n",
    "        - learning_rate (float): 学习率，用于优化器（Adam）。\n",
    "        - gamma (float): 折扣因子，用于计算折扣回报，范围 [0, 1]。\n",
    "        - device (torch.device): 计算设备（CPU 或 GPU，如 torch.device('cuda')）。\n",
    "        \"\"\"\n",
    "        super().__init__()  # 调用父类 torch.nn.Module 的构造函数，初始化模块\n",
    "        # 初始化策略网络 PolicyNet，用于参数化策略 π_θ(a|s)\n",
    "        # state_dim -> hidden_dim -> action_range\n",
    "        self.policynet = PolicyNet(state_dim, hidden_dim, action_range).to(device)\n",
    "        # 使用 Adam 优化器优化策略网络参数\n",
    "        # lr=learning_rate 指定学习率 α，用于梯度上升更新参数\n",
    "        self.optimizer = torch.optim.Adam(self.policynet.parameters(), lr=learning_rate)\n",
    "        # 折扣因子 γ，用于计算折扣回报 G_t\n",
    "        self.gamma = gamma\n",
    "        # 计算设备，确保张量和网络在同一设备上（CPU 或 GPU）\n",
    "        self.device = device\n",
    "        \n",
    "    def take_action(self, state):\n",
    "        \"\"\"\n",
    "        根据当前策略 π_θ(a|s) 从给定状态中采样动作。\n",
    "\n",
    "        参数：\n",
    "        - state (list or np.ndarray): 当前状态，通常是一个一维数组，形状为 (state_dim,)。\n",
    "\n",
    "        返回：\n",
    "        - int: 采样得到的动作索引（离散动作）。\n",
    "        \"\"\"\n",
    "        # 将输入状态转换为 PyTorch 张量，并指定数据类型为浮点数\n",
    "        # 示例：state = [0.5, 0.3, 1.0, -0.2] -> tensor([0.5, 0.3, 1.0, -0.2])\n",
    "        state = torch.tensor(state, dtype=torch.float).to(self.device)\n",
    "        # 增加批次维度，形状从 (state_dim,) 变为 (1, state_dim)\n",
    "        # 神经网络（如 PolicyNet）通常期望输入是批量形式 (batch_size, input_dim)\n",
    "        # 示例：tensor([0.5, 0.3, 1.0, -0.2]) -> tensor([[0.5, 0.3, 1.0, -0.2]])\n",
    "        state = state.unsqueeze(0)\n",
    "        # 通过策略网络计算动作概率分布 π_θ(a|s)，输出形状为 (1, action_range)\n",
    "        # squeeze() 移除批次维度，形状变为 (action_range,)\n",
    "        # 示例：tensor([[0.33, 0.33, 0.34]]) -> tensor([0.33, 0.33, 0.34])\n",
    "        probs = self.policynet(state).squeeze()\n",
    "        # 使用分类分布（Categorical Distribution）表示离散动作概率分布\n",
    "        # torch.distributions.Categorical 需要一维概率向量，probs 的和为 1（由 softmax 保证）\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        # 从概率分布中采样一个动作，action 是一个标量张量\n",
    "        # 示例：若 probs=[0.33, 0.33, 0.34]，action 可能是 tensor(2)（以 0.34 的概率采样到动作 2）\n",
    "        action = action_dist.sample()\n",
    "        # 将张量转换为 Python 整数，方便传递给环境\n",
    "        # 示例：tensor(2) -> 2\n",
    "        return action.item()\n",
    "    \n",
    "    def update(self, transition_dict):\n",
    "        \"\"\"\n",
    "        根据一条轨迹更新策略网络参数，使用 REINFORCE 算法的策略梯度方法。\n",
    "\n",
    "        参数：\n",
    "        - transition_dict (dict): 包含一条轨迹的数据，键包括：\n",
    "            - 'rewards': 奖励列表 [r_0, r_1, ..., r_T]\n",
    "            - 'states': 状态列表 [s_0, s_1, ..., s_T]\n",
    "            - 'actions': 动作列表 [a_0, a_1, ..., a_T]\n",
    "\n",
    "        数学原理：\n",
    "        - 目标函数 J(θ) = E[R(τ)]，其中 R(τ) 是轨迹总回报\n",
    "        - 策略梯度定理：∇_θ J(θ) = E[Σ_t ∇_θ log π_θ(a_t|s_t) * G_t]\n",
    "        - G_t 是从时间步 t 开始的折扣回报：G_t = r_t + γ r_{t+1} + γ^2 r_{t+2} + ...\n",
    "        \"\"\"\n",
    "        # 从字典中提取相关变量\n",
    "        # 奖励列表 [r_0, r_1, ..., r_T]\n",
    "        reward_list = transition_dict['rewards']\n",
    "        # 状态列表 [s_0, s_1, ..., s_T]\n",
    "        state_list = transition_dict['states']\n",
    "        # 动作列表 [a_0, a_1, ..., a_T]\n",
    "        action_list = transition_dict['actions']\n",
    "        # 初始化折扣回报 G 为 0，用于递归计算 G_t\n",
    "        G = 0\n",
    "        # 清空优化器的梯度缓存，为本次更新准备\n",
    "        # 避免上一次更新的梯度干扰\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # 从轨迹最后一步（t=T）向前遍历到第一步（t=0）\n",
    "        # 逆序计算折扣回报 G_t = r_t + γ r_{t+1} + γ^2 r_{t+2} + ...\n",
    "        # reversed() 反转序列，例如 range(5) -> [4, 3, 2, 1, 0]\n",
    "        for i in reversed(range(len(reward_list))):\n",
    "            # 获取当前时间步 t=i 的奖励 r_t\n",
    "            # 示例：reward_list=[1.0, 0.5, -0.2, 2.0, 3.0]，i=4 -> reward=3.0\n",
    "            reward = reward_list[i]\n",
    "            # 将当前状态 s_t 转换为 PyTorch 张量，并移动到指定设备\n",
    "            # 形状为 (state_dim,)，例如 tensor([0.5, 0.3, 1.0, -0.2])\n",
    "            state = torch.tensor(state_list[i], dtype=torch.float).to(self.device)\n",
    "            # 增加批次维度，形状从 (state_dim,) 变为 (1, state_dim)\n",
    "            # 适配 PolicyNet 的输入要求\n",
    "            # 然后计算动作概率分布 π_θ(a|s_t)，输出形状为 (1, action_range)\n",
    "            # squeeze() 移除批次维度，形状变为 (action_range,)\n",
    "            # 示例：probs = tensor([0.33, 0.33, 0.34])\n",
    "            probs = self.policynet(state.unsqueeze(0)).squeeze()\n",
    "            # 获取当前时间步的动作 a_t\n",
    "            # 示例：action_list=[0, 1, 2, 1, 0]，i=4 -> action=0\n",
    "            action = action_list[i]\n",
    "            # 计算动作 a_t 的对数概率 log π_θ(a_t|s_t)\n",
    "            # probs[action] 获取第 action 个动作的概率\n",
    "            # torch.log() 计算自然对数\n",
    "            # 示例：若 probs[0]=0.33，则 log_prob=torch.log(0.33)≈-1.1086\n",
    "            log_prob = torch.log(probs[action])\n",
    "            # 递归计算折扣回报 G_t\n",
    "            # G_t = r_t + γ G_{t+1}\n",
    "            # 示例：若 gamma=0.99，G=0（初始），reward=3.0，则 G=3.0\n",
    "            G = self.gamma * G + reward  \n",
    "            # 计算当前时间步的损失\n",
    "            # loss = -log π_θ(a_t|s_t) * G_t\n",
    "            # 负号是因为 PyTorch 优化器执行梯度下降，而我们需要梯度上升\n",
    "            # 梯度：∇_θ loss = -∇_θ log π_θ(a_t|s_t) * G_t，与策略梯度定理一致\n",
    "            loss = -log_prob * G\n",
    "            # 反向传播，计算损失对策略网络参数的梯度\n",
    "            # 梯度会累积到 self.policynet.parameters() 的 .grad 属性中\n",
    "            loss.backward()\n",
    "        \n",
    "        # 使用累积的梯度更新策略网络参数\n",
    "        # Adam 优化器执行一步梯度下降：θ = θ - α * ∇_θ loss\n",
    "        # 由于 loss 中有负号，实际执行的是梯度上升：θ = θ + α * ∇_θ J(θ)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
