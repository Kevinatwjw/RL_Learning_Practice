{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5f8e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce187c",
   "metadata": {},
   "source": [
    "\n",
    "#### 经验回放池："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b9114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_batch_size):\n",
    "        # deque定义的是一个先入先出对列，其效率高于.pop(0)\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.buffer = deque(maxlen=self.max_batch_size)  # 固定容量\n",
    "        \n",
    "    def push_transition(self, transition):\n",
    "        # 这里的实现没有判断是否重复（去重操作适合探索性任务）\n",
    "        self.buffer.append(transition)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        if batch_size > self.max_batch_size:\n",
    "            raise ValueError(\"采样的长度大于经验回放池大小！\")\n",
    "        return random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "    def len_buffer(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edbac16",
   "metadata": {},
   "source": [
    "#### DQN:\n",
    "1. DQN 算法概述\n",
    "DQN 是一种深度强化学习算法，结合 Q-Learning 和 深度神经网络，用于处理高维状态空间（如图像或连续状态）。由 DeepMind 在 2013 和 2015 年提出，广泛应用于 Atari 游戏等任务。DQN 使用神经网络近似 Q 值函数 $Q(s, a; \\theta)$，通过优化时序差分（TD）误差学习最优策略。\n",
    "\n",
    "2. 数学原理\n",
    "2.1 强化学习与马尔可夫决策过程 (MDP)\n",
    "- **强化学习建模为 MDP $(S, A, P, R, \\gamma)$**：\n",
    "状态 $S$：RollingBall 的 $[x, y, v_x, v_y]$。\n",
    "动作 $A$：25 个离散动作（Hashposotion.py）。\n",
    "转移概率 $P(s' | s, a)$：由 GridWorld.py 物理规则隐式定义。\n",
    "奖励 $R(s, a, s')$：-2.0（每步）、-10.0（撞墙）、+300.0（目标）。\n",
    "折扣因子 $\\gamma$：通常 0.99。\n",
    "\n",
    "- **目标：最大化累积折扣奖励**：\n",
    "$$\n",
    "J(\\pi) = \\mathbb{E}{\\pi} \\left[ \\sum{t=0}^\\infty \\gamma^t R(s_t, a_t, s_{t+1}) \\right]\n",
    "$$\n",
    "平稳分布 $d_\\pi(s)$，满足 $d_\\pi^T P_\\pi = d_\\pi^T$，描述策略 $\\pi$ 下的长期状态访问概率。\n",
    "\n",
    "2.2 Q-Learning 与 Bellman 方程\n",
    "Q 值函数 $Q^\\pi(s, a)$ 表示状态 $s$ 下执行动作 $a$，然后按策略 $\\pi$ 行动的预期累积奖励：\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}{\\pi} \\left[ R(s, a, s') + \\gamma \\sum{a'} \\pi(a'|s') Q^\\pi(s', a') \\right]$$\n",
    "最优 Q 值函数 $Q^*(s, a)$ 满足 Bellman 最优方程（《Chapter 8》, Box 7.5）：\n",
    "$$Q^(s, a) = \\mathbb{E} \\left[ R(s, a, s') + \\gamma \\max_{a'} Q^(s', a') \\right]$$\n",
    "Q-Learning 更新：\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ R(s, a, s') + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]$$\n",
    "RollingBall 的连续状态使 Q 表格不可行，需函数逼近。\n",
    "2.3 DQN 的函数逼近\n",
    "DQN 使用神经网络 $Q(s, a; \\theta)$ 近似 $Q^*(s, a)$。\n",
    "\n",
    "表格法：存储所有 $Q(s, a)$，对连续状态不可行。\n",
    "函数逼近：用参数 $\\theta$ 表示 $Q(s, a; \\theta)$，存储效率高，泛化能力强。\n",
    "\n",
    "RollingBall 配置：\n",
    "\n",
    "输入：4 维状态 $[x, y, v_x, v_y]$。\n",
    "输出：25 维 Q 值向量。\n",
    "网络：全连接网络（2-3 层，128 神经元，ReLU 激活）。\n",
    "\n",
    "DQN 优化 Bellman 误差：\n",
    "$$J(\\theta) = \\mathbb{E} \\left[ \\left( R + \\gamma \\max_{a'} Q(S', a'; \\theta) - Q(S, A; \\theta) \\right)^2 \\right]$$\n",
    "直接优化不稳定，需以下技术。\n",
    "3. DQN 的关键技术及数学推导\n",
    "3.1 经验回放（Experience Replay）\n",
    "问题：经验 $(s, a, r, s', \\text{done})$ 具有时间相关性，违背神经网络训练的 i.i.d. 假设。\n",
    "解决方案：回放池存储经验，随机采样批量数据。强调均匀采样确保状态-动作对 $(S, A)$ 近似均匀分布。\n",
    "数学原理：\n",
    "\n",
    "回放池 $\\mathcal{D}$ 存储 $(s, a, r, s', \\text{done})$。\n",
    "采样 $N$ 个经验，损失为：\n",
    "\n",
    "$$L(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\left[ r_i + \\gamma (1 - \\text{done}i) \\max{a'} Q(s'_i, a'; \\theta) - Q(s_i, a_i; \\theta) \\right]^2$$\n",
    "\n",
    "3.2 目标网络（Target Network）\n",
    "问题：目标值 $r + \\gamma \\max_{a'} Q(s', a'; \\theta)$ 随 $\\theta$ 变化，训练不稳定。\n",
    "解决方案：目标网络 $Q(s, a; \\theta^-)$，参数 $\\theta^-$ 定期复制。《Chapter 8》（Section 8.4.1）描述固定 $\\theta^-$ 简化梯度计算：\n",
    "$$L(\\theta) = \\mathbb{E} \\left[ \\left( r + \\gamma \\max_{a'} Q(s', a'; \\theta^-) - Q(s, a; \\theta) \\right)^2 \\right]$$\n",
    "数学推导：\n",
    "\n",
    "目标值：\n",
    "\n",
    "$$y_i = r_i + \\gamma (1 - \\text{done}i) \\max{a'} Q(s'_i, a'; \\theta^-)$$\n",
    "\n",
    "梯度：\n",
    "\n",
    "$$\\nabla_\\theta L(\\theta) = -\\frac{2}{N} \\sum_{i=1}^N \\left[ y_i - Q(s_i, a_i; \\theta) \\right] \\nabla_\\theta Q(s_i, a_i; \\theta)$$\n",
    "\n",
    "每 $k$ 步（如 100），$\\theta^- \\leftarrow \\theta$。\n",
    "\n",
    "\n",
    "3.3 ε-贪婪策略\n",
    "问题：需平衡探索与利用。\n",
    "解决方案：以概率 $\\epsilon$ 随机选择动作，否则选择 $\\arg\\max_a Q(s, a; \\theta)$。《Chapter 8》（Box 8.1）提到探索策略（如 ε-贪婪）确保平稳分布唯一。\n",
    "数学原理：\n",
    "\n",
    "策略：\n",
    "\n",
    "$$\\pi(a|s) = \\begin{cases}\\frac{\\epsilon}{|A|} + (1 - \\epsilon) \\cdot \\mathbb{I}[a = \\arg\\max_{a'} Q(s, a'; \\theta)] & \\text{if } a = \\arg\\max_{a'} Q(s, a'; \\theta) \\\\frac{\\epsilon}{|A|} & \\text{otherwise}\\end{cases}$$\n",
    "\n",
    "衰减：\n",
    "\n",
    "$$\\epsilon_t = \\epsilon_{\\text{end}} + (\\epsilon_{\\text{start}} - \\epsilon_{\\text{end}}) \\exp(-t / \\tau)$$\n",
    "与 RollingBall 的结合：\n",
    "\n",
    "env.action_space.sample() 实现随机动作。\n",
    "衰减 $\\epsilon$（如 1.0 到 0.1）鼓励目标探索。\n",
    "\n",
    "4. DQN 算法流程\n",
    "综合《Chapter 8》（Algorithm 8.3）和 hrl.boyuai.com：\n",
    "\n",
    "初始化：\n",
    "\n",
    "在线网络 $Q(s, a; \\theta)$，目标网络 $Q(s, a; \\theta^-)$，$\\theta^- \\leftarrow \\theta$。\n",
    "回放池 $\\mathcal{D}$，容量 capacity。\n",
    "超参数：学习率 $\\alpha$，折扣因子 $\\gamma$，$\\epsilon$ 参数。\n",
    "\n",
    "交互：\n",
    "\n",
    "按 ε-贪婪选择 $a$。\n",
    "执行 $a$，获取 $r, s', \\text{done}$。\n",
    "存储 $(s, a, r, s', \\text{done})$。\n",
    "\n",
    "优化：\n",
    "\n",
    "采样批量经验。\n",
    "\n",
    "目标值：\n",
    "$$y_i = r_i + \\gamma (1 - \\text{done}i) \\max{a'} Q(s'_i, a'; \\theta^-)$$\n",
    "\n",
    "损失：\n",
    "$$L(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\left[ y_i - Q(s_i, a_i; \\theta) \\right]^2$$\n",
    "\n",
    "更新：\n",
    "\n",
    "每 $k$ 步，$\\theta^- \\leftarrow \\theta$。\n",
    "衰减 $\\epsilon_t$。\n",
    "\n",
    "5. 局限与改进\n",
    "\n",
    "Q 值过估计：\n",
    "\n",
    "$\\max_{a'} Q(s', a'; \\theta^-)$ 可能高估。《Chapter 10》（Section 10.2）提到基线减少方差，DQN 用 Double DQN：\n",
    "$$y_i = r_i + \\gamma (1 - \\text{done}_i) Q(s'i, \\arg\\max{a'} Q(s'_i, a'; \\theta); \\theta^-)$$\n",
    "\n",
    "稀疏奖励：RollingBall 的 +300.0 奖励稀疏，需奖励整形。\n",
    "\n",
    "离散动作：DQN 限于离散动作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c33fc1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_net(torch.nn.Module):\n",
    "    \"\"\"\"Q网络使两个MLP进行连接，用于DQN和Double DQN\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        # 调用nn.Module的初始化，确保网络能够正确初始化\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        # 一维批归一化（LayerNorm Normalization）层的操作\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"对每一层进行权重初始化\"\"\"\n",
    "        # 隐藏层：使用 ReLU 专用的 Kaiming 初始化\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n",
    "        # 初始化全连接层偏置，稳定初始输出，加速早期训练。\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.kaiming_uniform_(self.fc2.weight, nonlinearity='relu')\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "        # 输出层受限初始化\n",
    "        bound = 3 / np.sqrt(self.fc3.weight.size(0))\n",
    "        nn.init.uniform_(self.fc3.weight, -bound, bound)\n",
    "        nn.init.zeros_(self.fc3.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"定义从输入状态到输出 Q 值的计算流程。\n",
    "            输入 x：状态张量，形状为 (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        x = self.ln1(F.relu(self.fc1(x)))\n",
    "        x = self.ln2(F.relu(self.fc2(x)))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd96e447",
   "metadata": {},
   "source": [
    "1. self.bn1 = nn.BatchNorm1d(hidden_dim) 是定义一个一维批归一化（Batch Normalization）层的操作。\n",
    "- **核心功能**针对全连接层（Linear）或一维卷积层（Conv1D）的输出.对每个隐藏层神经元的输出进行标准化处理：\n",
    "    $$\n",
    "    \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\quad \\text{(标准化)}\n",
    "    $$\n",
    "    $$\n",
    "    y_i = \\gamma \\hat{x}_i + \\beta \\quad \\text{(可学习缩放和平移)}\n",
    "    $$\n",
    "  其中：\n",
    "$μ_B$是该神经元在当前batch上的均值,$σ_B^2$是方差,γ(weight)和β(bias)是可学习的参数。\n",
    "\n",
    "- **在DQN中的具体作用:**\n",
    "\n",
    "    正向传播时：标准化过程计算当前batch中每个神经元的均值和方差，对128个神经元的输出独立标准化，通过γ和β保留网络的表达能力\n",
    "\n",
    "    反向传播时：自动更新当前batch的$μ_B$和$σ_B^2$、可学习参数γ和β、全局统计量（用于推理的running_mean和running_var）\n",
    "\n",
    "- **数据流维度变化：**\n",
    "\n",
    "    输入: [batch_size, 4] \n",
    "    \n",
    "    fc1: [batch_size, 128] \n",
    "\n",
    "    bn1: [batch_size, 128] (标准化后)\n",
    "\n",
    "    fc2: [batch_size, 128]\n",
    "\n",
    "    输出: [batch_size, 25]\n",
    "    通过这种设计，BatchNorm1d能有效提升DQN在RollingBall环境中的训练稳定性和收敛速度。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6909c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, action_range, lr, gamma, \n",
    "                 epsilon, device, seed=None):\n",
    "        super().__init__()\n",
    "        # 初始化参数\n",
    "        self.action_dim = action_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.action_range = action_range # 动作范围\n",
    "        self.epsilon = epsilon # spsilon-greedy探索率\n",
    "        self.gamma = gamma # 折扣率\n",
    "        self.lr = lr\n",
    "        self.rng = np.random.RandomState(seed) \n",
    "        self.device = device\n",
    "        # 初始化训练网络\n",
    "        self.Qnet = Q_net(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # 初始化目标网络\n",
    "        self.Qnet_target = Q_net(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # 初始化使用Adam更新器(仅优化在线训练网络)\n",
    "        self.optimizer = torch.optim.Adam(self.Qnet.parameters(), lr=lr)\n",
    "    \n",
    "    def max_q_value_of_state(self, state):\n",
    "        \"\"\"计算给定状态下所有动作的最大的Q值，评估策略质量\"\"\"\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        return self.Qnet(state).max().item()\n",
    "\n",
    "    def take_action(self, state):\n",
    "        \"\"\"采取动作,使用ε-greedy策略\"\"\"\n",
    "        if self.rng.random() < self.epsilon:\n",
    "            action = self.rng.randint(self.action_range)\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float,).to(self.device)\n",
    "            action = self.Qnet(state).argmax().item()\n",
    "        return action\n",
    "    \n",
    "    def update(self, transition_dict, tau=0.01):\n",
    "        # 将经验转化成张量：states,next_states-[batch_size, state_dim],actions-[batch_size, 1],rewards,done-[(batch_size)]\n",
    "        states = torch.tensor(transition_dict['states'], dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions'], dtype=torch.int64).view(-1, 1).to(self.device)\n",
    "        # 创建张量->升维，[batch_size, 1]（该形式符合自动广播机制）->设备转移->降维tensor([1,2,3])\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float32).view(-1,1).to(self.device).squeeze()\n",
    "        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float32).view(-1,1).to(self.device).squeeze()\n",
    "        \n",
    "        # Q值计算and目标Q值计算\n",
    "        # 计算当前状态-动作对的 Q 值 Q(s_i, a_i; θ)。\n",
    "        q_values = self.Qnet(states).gather(dim=1, index = actions).squeeze()\n",
    "        max_next_q_valus = self.Qnet_target(next_states).max(axis=1)[0] # 取每行最大值\n",
    "        # 结合未来奖励与未来Q值，并考虑是否终止\n",
    "        q_targets = rewards + self.gamma * max_next_q_valus * (1 - dones)\n",
    "        \n",
    "        # 计算训练损失(q_values，q_targets均为（batch_size,actions_dim）)\n",
    "        DQN_Loss = F.mse_loss(q_values, q_targets, reduction='mean') # 计算 Q 值与目标的 MSE.（默认自动平均）\n",
    "        self.optimizer.zero_grad() # 清空梯度,避免梯度累加（PyTorch默认会累加梯度，训练时必须手动清零）\n",
    "        DQN_Loss.backward() # 自动计算损失对网络参数的梯度,梯度存储在参数的 .grad 属性中\n",
    "        self.optimizer.step() # 根据梯度更新网络\n",
    "        \n",
    "        # 目标网络更新(采用软更新)(网络中使用layernorm，如果使用batchnorm需要将running.mean和runnig.var进行软更新)\n",
    "        for target_param, param in zip(self.Qnet_target.parameters(), self.Qnet.parameters()):\n",
    "            target_param.data.copy_(tau * param + (1-tau)*target_param)\n",
    "        \"\"\"\n",
    "        若网络同时包含batchnorm和layernorm:使用一下方案\n",
    "        for target_param, param in zip(self.Qnet_target.modules(), self.Qnet.modules()):\n",
    "            # 更新普通参数\n",
    "            if hasattr(target_param, 'weight') and not isinstance(target_param, (nn.BatchNorm1d, nn.LayerNorm)):\n",
    "                target_param.weight.data.copy_(tau*param.weight + (1-tau)*target_param.weight)\n",
    "            # 更新BatchNorm的running统计量\n",
    "            if isinstance(tgtarget_paramt, nn.BatchNorm1d):\n",
    "                target_param.running_mean.copy_(tau*param.running_mean + (1-tau)*target_param.running_mean)\n",
    "                target_param.running_var.copy_(tau*param.running_var + (1-tau)*target_param.running_var)\n",
    "        \"\"\"\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
