{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4252c4d2",
   "metadata": {},
   "source": [
    "RollingBall:玩家（或智能体）通过施加力控制小球，从网格起点（[0.2*宽, 0.2*高]）移动到目标点（[0.8*宽, 0.8*高]）。\n",
    "\n",
    "核心特点：\n",
    "- **环境**：10x10 网格（可调），小球受力、摩擦（系数 0.0046）、边界碰撞（恢复系数 0.8）影响，速度上限 5.0。\n",
    "- **动作**：连续力（[-0.1, 0.1]），可离散化为 5x5 动作（[-0.8, -0.4, 0, 0.4, 0.8]）并展平为 25 个动作。\n",
    "- **奖励**：每步 -2.0，撞墙 -10.0，到达目标 +300.0。\n",
    "- **渲染**：Pygame 显示蓝色小球、紫色目标，可选灰色轨迹，300x300 像素窗口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93377cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from gym.utils.env_checker import check_env\n",
    "from RL_DQN_Class import DQN,ReplayBuffer\n",
    "from gym.wrappers import TimeLimit\n",
    "from two_dimensional_rolling_motion import RollingBall, DiscreteActionWrapper, FlattenActionSpaceWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381e5f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置 PyTorch 默认浮点类型为 torch.float32，确保张量数据类型一致，防止类型不匹配错误\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# 定义移动平均函数，用于平滑回报曲线，便于观察训练趋势\n",
    "def moving_average(a, window_size):\n",
    "    \"\"\"\n",
    "    计算数组 a 的移动平均，用于平滑数据\n",
    "    Args:\n",
    "        a (array-like): 输入数据（如回报列表）\n",
    "        window_size (int): 移动平均窗口大小\n",
    "    Returns:\n",
    "        np.ndarray: 平滑后的数据，长度与输入相同\n",
    "    \"\"\"\n",
    "    result = np.convolve(a, np.ones(window_size)/window_size, mode='valid')\n",
    "    pad_before = (window_size - 1) // 2\n",
    "    pad_after = window_size - 1 - pad_before\n",
    "    return np.pad(result, (pad_before, pad_after), mode='edge')\n",
    "\n",
    "# 定义设置随机种子的函数，确保实验可重复\n",
    "def set_seed(env, seed=42):\n",
    "    \"\"\"\n",
    "    设置环境和全局随机种子，确保训练结果可复现\n",
    "    Args:\n",
    "        env: Gym 环境对象\n",
    "        seed (int): 随机种子，默认为 42\n",
    "    \"\"\"\n",
    "    env.action_space.seed(seed)\n",
    "    env.reset(seed=seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 定义超参数\n",
    "    state_dim = 4  # 状态维度（x位置, y位置, x速度, y速度）\n",
    "    action_bins = 10  # 动作空间离散化粒度（每个维度10个离散动作）\n",
    "    action_range = action_bins * action_bins  # 总动作数（10x10=100）\n",
    "    action_dim = action_range  # 动作维度，与 action_range 一致\n",
    "    hidden_dim = 32  # 神经网络隐藏层维度\n",
    "    lr = 1e-3  # 学习率\n",
    "    num_episodes = 1000  # 总训练回合数\n",
    "    gamma = 0.99  # 折扣因子，决定未来奖励的重要性\n",
    "    epsilon_start = 0.1  # 初始 epsilon 值，用于 ε-greedy 策略的探索\n",
    "    epsilon_end = 0.001  # 最终 epsilon 值，探索率衰减的目标\n",
    "    buffer_size = 10000  # 经验回放池最大容量\n",
    "    minimal_size = 5000  # 开始训练前回放池的最小样本数\n",
    "    batch_size = 128  # 每次训练的批量大小\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")  # 选择 GPU 或 CPU\n",
    "\n",
    "    # 构建环境\n",
    "    env = RollingBall(width=5, height=5, show_epi=True)  # 创建 RollingBall 环境，5x5 网格，显示轨迹\n",
    "    env = FlattenActionSpaceWrapper(DiscreteActionWrapper(env, bins=10))  # 离散化动作空间并展平为 1D\n",
    "    env = TimeLimit(env, 100)  # 设置最大步数限制为 100 步\n",
    "    check_env(env.unwrapped)  # 验证环境是否符合 Gym 规范\n",
    "    set_seed(env, seed=42)  # 设置随机种子\n",
    "\n",
    "    # 验证环境输出\n",
    "    state, _ = env.reset()  # 重置环境，获取初始状态\n",
    "    print(f\"Initial state: {state}, dtype: {state.dtype}\")  # 打印初始状态和数据类型，确保为 np.float32\n",
    "\n",
    "    # 构建智能体\n",
    "    replay_buffer = ReplayBuffer(buffer_size)  # 创建经验回放池\n",
    "    agent = DQN(state_dim, hidden_dim, action_dim, action_range, lr, gamma, epsilon_start, device)  # 创建 DQN 智能体\n",
    "    print(f\"Q_net weight dtype: {next(agent.Qnet.parameters()).dtype}\")  # 打印 Q 网络参数类型，确保为 torch.float32\n",
    "\n",
    "    # 填充经验回放池\n",
    "    state, _ = env.reset()  # 重置环境\n",
    "    while replay_buffer.len_buffer() <= minimal_size:\n",
    "        action = env.action_space.sample()  # 随机采样动作\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)  # 执行动作\n",
    "        replay_buffer.push_transition((state, action, reward, next_state, terminated or truncated))  # 存储经验\n",
    "        state = next_state\n",
    "        if terminated or truncated:\n",
    "            state, _ = env.reset()  # 如果回合结束，重置环境\n",
    "    print(f\"Replay buffer filled with {replay_buffer.len_buffer()} transitions\")  # 打印回放池填充状态\n",
    "\n",
    "    # 训练\n",
    "    return_list = []  # 存储每个回合的总回报\n",
    "    max_q_value_list = []  # 存储最大 Q 值的历史\n",
    "    max_q_value = 0  # 初始化最大 Q 值\n",
    "    epsilon_decay = (epsilon_start - epsilon_end) / num_episodes  # 计算 epsilon 线性衰减率\n",
    "    os.makedirs('./result', exist_ok=True)  # 创建结果保存目录\n",
    "\n",
    "    for i in range(20):  # 分 20 次迭代，每迭代训练 num_episodes/20 回合\n",
    "        with tqdm(total=int(num_episodes / 20), desc='Iteration %d' % i) as pbar:  # 使用 tqdm 显示进度条\n",
    "            for i_episode in range(int(num_episodes / 20)):\n",
    "                # 更新 epsilon 值，线性衰减以减少探索\n",
    "                agent.epsilon = max(epsilon_end, epsilon_start - (i * num_episodes / 20 + i_episode) * epsilon_decay)\n",
    "                episode_return = 0  # 初始化回合总回报\n",
    "                episode_steps = 0  # 初始化回合步数\n",
    "                state, _ = env.reset()  # 重置环境\n",
    "                while True:\n",
    "                    # 计算平滑的最大 Q 值，用于监控 Q 值趋势\n",
    "                    max_q_value = agent.max_q_value_of_state(state) * 0.005 + max_q_value * 0.995\n",
    "                    max_q_value_list.append(max_q_value)\n",
    "                    action = agent.take_action(state)  # 选择动作（ε-greedy 策略）\n",
    "                    next_state, reward, terminated, truncated, info = env.step(action)  # 执行动作\n",
    "                    replay_buffer.push_transition((state, action, reward, next_state, terminated or truncated))  # 存储经验\n",
    "                    episode_steps += 1  # 步数加 1\n",
    "\n",
    "                    # 每 100 步打印详细日志\n",
    "                    if episode_steps % 100 == 0:\n",
    "                        print(f\"Iteration {i}, Episode {i_episode + 1}, Step {episode_steps}: \"\n",
    "                              f\"State = {state}, Action = {action}, Reward = {reward}, \"\n",
    "                              f\"Next State = {next_state}, Distance = {info.get('distance_to_target', 'N/A')}, \"\n",
    "                              f\"Epsilon = {agent.epsilon:.4f}, Max Q = {max_q_value:.4f}, \"\n",
    "                              f\"State dtype = {state.dtype}\")\n",
    "\n",
    "                    # 当回放池足够大时，开始训练\n",
    "                    if replay_buffer.len_buffer() > minimal_size:\n",
    "                        samples = replay_buffer.sample(batch_size)  # 采样批量经验\n",
    "                        states, actions, rewards, next_states, dones = zip(*samples)  # 解包经验\n",
    "                        # 创建训练数据，确保数据类型为 np.float32\n",
    "                        transition_dict = {\n",
    "                            'states': np.array(states, dtype=np.float32),\n",
    "                            'actions': np.array(actions, dtype=np.int64),\n",
    "                            'next_states': np.array(next_states, dtype=np.float32),\n",
    "                            'rewards': np.array(rewards, dtype=np.float32),\n",
    "                            'dones': np.array(dones, dtype=np.float32)\n",
    "                        }\n",
    "                        agent.update(transition_dict)  # 更新 Q 网络\n",
    "\n",
    "                    state = next_state  # 更新当前状态\n",
    "                    episode_return += reward  # 累加回合回报\n",
    "\n",
    "                    # 如果回合结束（终止或截断）\n",
    "                    if terminated or truncated:\n",
    "                        # 每 10 回合打印回合总结\n",
    "                        if (i_episode + 1) % 10 == 0:\n",
    "                            print(f\"Iteration {i}, Episode {i_episode + 1}: \"\n",
    "                                  f\"Return = {episode_return:.2f}, Steps = {episode_steps}, \"\n",
    "                                  f\"Avg Return (last 10) = {np.mean(return_list[-10:]):.2f}, \"\n",
    "                                  f\"Buffer Size = {replay_buffer.len_buffer()}, \"\n",
    "                                  f\"Target Distance = {info.get('distance_to_target', 'N/A'):.2f}\")\n",
    "                            env.render()  # 渲染环境\n",
    "                        break\n",
    "\n",
    "                return_list.append(episode_return)  # 记录回合回报\n",
    "                # 更新进度条显示\n",
    "                if (i_episode + 1) % 10 == 0:\n",
    "                    pbar.set_postfix({\n",
    "                        'episode': '%d' % (num_episodes / 20 * i + i_episode + 1),\n",
    "                        'return': '%.3f' % np.mean(return_list[-10:]),\n",
    "                        'steps': '%d' % episode_steps\n",
    "                    })\n",
    "                pbar.update(1)\n",
    "\n",
    "    # 绘制训练结果\n",
    "    mv_return_list = moving_average(return_list, 29)  # 计算回报的移动平均\n",
    "    episodes_list = list(range(len(return_list)))\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(episodes_list, return_list, label='raw', alpha=0.5)  # 原始回报曲线\n",
    "    plt.plot(episodes_list, mv_return_list, label='moving ave')  # 平滑回报曲线\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Returns')\n",
    "    plt.title('DQN on RollingBall')\n",
    "    plt.legend()\n",
    "    plt.savefig('./result/DQN.png')  # 保存回报曲线\n",
    "    plt.show()\n",
    "\n",
    "    # 绘制最大 Q 值曲线\n",
    "    frames_list = list(range(len(max_q_value_list)))\n",
    "    plt.plot(frames_list, max_q_value_list)\n",
    "    plt.axhline(max(max_q_value_list), c='orange', ls='--')  # 标记最大 Q 值\n",
    "    plt.xlabel('Frames')\n",
    "    plt.ylabel('Max Q_value')\n",
    "    plt.title('DQN on RollingBall')\n",
    "    plt.savefig('./result/DQN_MaxQ.png')  # 保存 Q 值曲线\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
