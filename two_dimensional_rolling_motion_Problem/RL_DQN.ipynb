{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4252c4d2",
   "metadata": {},
   "source": [
    "RollingBall:玩家（或智能体）通过施加力控制小球，从网格起点（[0.2*宽, 0.2*高]）移动到目标点（[0.8*宽, 0.8*高]）。\n",
    "\n",
    "核心特点：\n",
    "- **环境**：10x10 网格（可调），小球受力、摩擦（系数 0.0046）、边界碰撞（恢复系数 0.8）影响，速度上限 5.0。\n",
    "- **动作**：连续力（[-0.1, 0.1]），可离散化为 5x5 动作（[-0.8, -0.4, 0, 0.4, 0.8]）并展平为 25 个动作。\n",
    "- **奖励**：每步 -2.0，撞墙 -10.0，到达目标 +300.0。\n",
    "- **渲染**：Pygame 显示蓝色小球、紫色目标，可选灰色轨迹，300x300 像素窗口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93377cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from gym.utils.env_checker import check_env\n",
    "from RL_DQN_Class import DQN,ReplayBuffer\n",
    "from gym.wrappers import TimeLimit\n",
    "from two_dimensional_rolling_motion import RollingBall, DiscreteActionWrapper, FlattenActionSpaceWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "381e5f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\py\\Anaconda3\\envs\\DL_pytorch_gpu\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "Iteration 0: 100%|██████████| 50/50 [00:12<00:00,  3.94it/s, episode=50, return=-239.000]\n",
      "Iteration 1: 100%|██████████| 50/50 [00:12<00:00,  3.87it/s, episode=150, return=-306.000]\n",
      "Iteration 2: 100%|██████████| 50/50 [00:11<00:00,  4.19it/s, episode=250, return=-248.000]\n",
      "Iteration 3: 100%|██████████| 50/50 [00:13<00:00,  3.78it/s, episode=350, return=-343.000]\n",
      "Iteration 4: 100%|██████████| 50/50 [00:12<00:00,  3.90it/s, episode=450, return=-247.000]\n",
      "Iteration 5: 100%|██████████| 50/50 [00:13<00:00,  3.84it/s, episode=550, return=-246.000]\n",
      "Iteration 6:  64%|██████▍   | 32/50 [00:08<00:04,  3.74it/s, episode=630, return=-227.000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 332.0: Goal reached!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 6:  84%|████████▍ | 42/50 [00:11<00:01,  4.76it/s, episode=640, return=-225.200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 342.0: Goal reached!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 6: 100%|██████████| 50/50 [00:13<00:00,  3.73it/s, episode=650, return=-268.400]\n",
      "Iteration 7:  14%|█▍        | 7/50 [00:02<00:13,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 357.0: Goal reached!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 7:  50%|█████     | 25/50 [00:06<00:06,  3.93it/s, episode=720, return=-270.000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 375.0: Goal reached!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 7:  70%|███████   | 35/50 [00:08<00:03,  4.53it/s, episode=730, return=-212.800]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 385.0: Goal reached!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 7:  78%|███████▊  | 39/50 [00:09<00:02,  5.09it/s, episode=730, return=-212.800]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 389.0: Goal reached!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 7:  84%|████████▍ | 42/50 [00:10<00:01,  4.53it/s, episode=740, return=-160.800]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 392.0: Goal reached!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 7:  90%|█████████ | 45/50 [00:11<00:01,  4.44it/s, episode=740, return=-160.800]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 394.0: Goal reached!\n",
      "Episode 395.0: Goal reached!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 7:  96%|█████████▌| 48/50 [00:11<00:00,  4.20it/s, episode=740, return=-160.800]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 398.0: Goal reached!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 7: 100%|██████████| 50/50 [00:12<00:00,  4.02it/s, episode=750, return=-103.200]\n",
      "Iteration 8:  22%|██▏       | 11/50 [00:02<00:09,  3.95it/s, episode=810, return=-260.000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 411.0: Goal reached!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 8: 100%|██████████| 50/50 [00:12<00:00,  3.95it/s, episode=850, return=-625.000]\n",
      "Iteration 9: 100%|██████████| 50/50 [00:13<00:00,  3.67it/s, episode=950, return=-241.000]\n",
      "Iteration 10: 100%|██████████| 50/50 [00:13<00:00,  3.72it/s, episode=1050, return=-240.000]\n",
      "Iteration 11:  48%|████▊     | 24/50 [00:06<00:08,  3.16it/s, episode=1120, return=-266.000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 574.0: Goal reached!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 11:  52%|█████▏    | 26/50 [00:07<00:06,  3.45it/s, episode=1120, return=-266.000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 576.0: Goal reached!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 11: 100%|██████████| 50/50 [00:13<00:00,  3.69it/s, episode=1150, return=-239.000]\n",
      "Iteration 12: 100%|██████████| 50/50 [00:13<00:00,  3.70it/s, episode=1250, return=-231.000]\n",
      "Iteration 13:   4%|▍         | 2/50 [00:00<00:11,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 652.0: Goal reached!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 13:  24%|██▍       | 12/50 [00:03<00:12,  3.10it/s, episode=1310, return=-239.000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 662.0: Goal reached!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 13:  46%|████▌     | 23/50 [00:05<00:06,  4.29it/s, episode=1320, return=-194.000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 673.0: Goal reached!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 13:  64%|██████▍   | 32/50 [00:07<00:03,  5.22it/s, episode=1330, return=-197.000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 681.0: Goal reached!\n",
      "Episode 682.0: Goal reached!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 13: 100%|██████████| 50/50 [00:12<00:00,  4.01it/s, episode=1350, return=-257.000]\n",
      "Iteration 14: 100%|██████████| 50/50 [00:13<00:00,  3.69it/s, episode=1450, return=-344.000]\n",
      "Iteration 15: 100%|██████████| 50/50 [00:13<00:00,  3.62it/s, episode=1550, return=-262.000]\n",
      "Iteration 16: 100%|██████████| 50/50 [00:12<00:00,  3.93it/s, episode=1650, return=-248.000]\n",
      "Iteration 17:  44%|████▍     | 22/50 [00:06<00:07,  3.62it/s, episode=1720, return=-254.000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 149\u001b[0m\n\u001b[0;32m    141\u001b[0m transition_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m'\u001b[39m: b_s,\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m'\u001b[39m: b_a,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdones\u001b[39m\u001b[38;5;124m'\u001b[39m: b_d\n\u001b[0;32m    147\u001b[0m }\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# 组织批量数据\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransition_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# 更新 Q 网络，计算 Q 学习损失并优化\u001b[39;00m\n\u001b[0;32m    152\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "File \u001b[1;32mf:\\Learning_project\\RL_Learning\\RL_Learning_Practice\\two_dimensional_rolling_motion_Problem\\RL_DQN_Class.py:87\u001b[0m, in \u001b[0;36mDQN.update\u001b[1;34m(self, transition_dict)\u001b[0m\n\u001b[0;32m     85\u001b[0m dqn_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(F\u001b[38;5;241m.\u001b[39mmse_loss(q_values, q_targets))  \n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()                                                         \n\u001b[1;32m---> 87\u001b[0m dqn_loss\u001b[38;5;241m.\u001b[39mbackward() \n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# 软更新目标网络参数\u001b[39;00m\n",
      "File \u001b[1;32md:\\py\\Anaconda3\\envs\\DL_pytorch_gpu\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\py\\Anaconda3\\envs\\DL_pytorch_gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    def moving_average(a, window_size):\n",
    "        ''' 生成序列 a 的滑动平均序列 '''\n",
    "        # 计算滑动平均，平滑序列（如回合回报），便于绘图时观察趋势\n",
    "        # 参数：a（输入序列，如回报列表），window_size（窗口大小，后续设为 29）\n",
    "        cumulative_sum = np.cumsum(np.insert(a, 0, 0))\n",
    "        # 计算累积和，插入 0 便于差分计算窗口平均值\n",
    "        middle = (cumulative_sum[window_size:] - cumulative_sum[:-window_size]) / window_size\n",
    "        # 中间部分：通过累积和差分计算窗口平均值\n",
    "        r = np.arange(1, window_size-1, 2)\n",
    "        # 生成奇数序列 [1, 3, 5, ...]，用于处理开头和结尾的小窗口\n",
    "        begin = np.cumsum(a[:window_size-1])[::2] / r\n",
    "        # 开头部分：计算前 window_size-1 个元素的小窗口平均值\n",
    "        end = (np.cumsum(a[:-window_size:-1])[::2] / r)[::-1]\n",
    "        # 结尾部分：计算最后 window_size-1 个元素的反向小窗口平均值，翻转对齐\n",
    "        return np.concatenate((begin, middle, end))\n",
    "        # 拼接开头、中间、结尾，返回等长滑动平均序列\n",
    "\n",
    "    def set_seed(env, seed=42):\n",
    "        ''' 设置随机种子 '''\n",
    "        # 设置随机种子，确保环境和算法行为一致，实验可重复\n",
    "        env.action_space.seed(seed)\n",
    "        # 为动作空间设置种子，确保动作采样一致\n",
    "        env.reset(seed=seed)\n",
    "        # 重置环境，应用种子，确保初始状态一致\n",
    "        random.seed(seed)\n",
    "        # 设置 Python random 模块种子，用于随机动作选择\n",
    "        np.random.seed(seed)\n",
    "        # 设置 NumPy 随机种子，用于数组操作和采样\n",
    "        torch.manual_seed(seed)\n",
    "        # 设置 PyTorch 种子，确保神经网络初始化和训练一致\n",
    "\n",
    "    state_dim = 4                               # 环境观测维度\n",
    "    # 状态维度为 4，对应 RollingBall 环境的观测 [x_position, y_position, x_velocity, y_velocity]\n",
    "    action_dim = 1                              # 环境动作维度\n",
    "    action_bins = 10                            # 动作离散 bins 数量\n",
    "    action_range = action_bins * action_bins    # 环境动作空间大小\n",
    "    hidden_dim = 128                             # 神经网络隐藏层大小\n",
    "    lr = 0.01                                   # 学习率\n",
    "    num_episodes = 1000                         # 总训练回合数\n",
    "    gamma = 0.99                                # 折扣因子\n",
    "    epsilon_start = 0.1                        # 初始探索率\n",
    "    epsilon_end = 0.05                        # 最终探索率\n",
    "    tau = 1000                        # 目标网络更新参数\n",
    "    buffer_size = 10000                         # 回放缓冲区容量\n",
    "    minimal_size = 5000                         # 训练前最小缓冲区大小\n",
    "    batch_size = 128                            # 批量大小\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    # 优先使用 GPU（CUDA），否则使用 CPU，加速神经网络计算\n",
    "\n",
    "    # 构建环境\n",
    "    env = RollingBall(width=5, height=5, show_epi=True)\n",
    "    # 创建 5x5 的 RollingBall 环境，显示小球轨迹\n",
    "    # 参考 GridWorld.py：\n",
    "    # - 初始位置：(1, 1)（0.2 * 5）\n",
    "    # - 目标位置：(4, 4)（0.8 * 5）\n",
    "    # - 奖励：每步 -2.0，撞墙 -10.0，目标 +300.0\n",
    "    # - 动作空间：Box([-0.1, 0.1], shape=(2,))，x 和 y 方向的力\n",
    "    # - 观测空间：Box([0, 0, -5, -5], [5, 5, 5, 5])，[x, y, vx, vy]\n",
    "    env = FlattenActionSpaceWrapper(DiscreteActionWrapper(env, bins=10))\n",
    "    # 包装动作空间：\n",
    "    # 1. DiscreteActionWrapper（参考 Hashposotion.py）：\n",
    "    #    - 将连续动作离散化为 MultiDiscrete([10, 10])\n",
    "    #    - 每个维度 10 个 bins，映射到 [-1, 1]，步长 0.2\n",
    "    # 2. FlattenActionSpaceWrapper：\n",
    "    #    - 展平为 Discrete(100)，动作索引 0-99\n",
    "    env = TimeLimit(env, 100)\n",
    "    # 限制每回合最大 100 步，超限返回 truncated=True\n",
    "    check_env(env.unwrapped)\n",
    "    # 检查原始 RollingBall 环境是否符合 Gym 规范\n",
    "    set_seed(env, seed=42)\n",
    "    # 设置环境随机种子，确保一致性\n",
    "\n",
    "    # 构建代理和回放缓冲区\n",
    "    replay_buffer = ReplayBuffer(buffer_size)\n",
    "    # 创建回放缓冲区，容量 10000，存储经验 (state, action, reward, next_state, done)\n",
    "    agent = DQN(state_dim, hidden_dim, action_dim, action_range, lr, gamma, epsilon_start, tau, device)\n",
    "    # 创建 DQN 代理（假设 DQN 类已定义）\n",
    "    # 配置：4 维状态输入，100 个动作输出，32 单元隐藏层，学习率 0.001，折扣因子 0.99\n",
    "    # 功能：epsilon-greedy 动作选择，Q 网络更新，目标网络定期同步\n",
    "\n",
    "    # 随机动作填充回放缓冲区\n",
    "    state, _ = env.reset()\n",
    "    # 重置环境，获取初始状态\n",
    "    while replay_buffer.size() <= minimal_size:\n",
    "        action = env.action_space.sample()\n",
    "        # 随机选择动作（0-99）\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        # 执行动作，获取下一状态、奖励、终止标志等\n",
    "        replay_buffer.add(state, action, reward, next_state, done=terminated or truncated)\n",
    "        # 存储经验，done 表示回合是否结束\n",
    "        state = next_state\n",
    "        # 更新状态\n",
    "        if terminated or truncated:\n",
    "            env.render()\n",
    "            # 回合结束时渲染，显示小球轨迹\n",
    "            state, _ = env.reset()\n",
    "            # 重置环境\n",
    "        #print(replay_buffer.size())\n",
    "        # （注释掉）可打印缓冲区大小，监控填充进度\n",
    "\n",
    "    # 开始训练\n",
    "    return_list = []\n",
    "    # 存储每回合总回报\n",
    "    max_q_value_list = []\n",
    "    # 存储平滑的最大 Q 值\n",
    "    max_q_value = 0\n",
    "    # 用于平滑 Q 值的初始值\n",
    "    for i in range(20):\n",
    "        # 分 20 次迭代，每迭代 50 回合，共 1000 回合\n",
    "        with tqdm(total=int(num_episodes / 20), desc='Iteration %d' % i) as pbar:\n",
    "            # 使用 tqdm 显示进度条\n",
    "            for i_episode in range(int(num_episodes / 20)):\n",
    "                # 每迭代 50 回合\n",
    "                episode_return = 0\n",
    "                # 初始化回合回报\n",
    "                state, _ = env.reset()\n",
    "                # 重置环境\n",
    "                while True:\n",
    "                    # 计算并平滑最大 Q 值\n",
    "                    max_q_value = agent.max_q_value_of_given_state(state) * 0.005 + max_q_value * 0.995\n",
    "                    # 使用指数平滑，权重 0.005（新值）和 0.995（旧值）\n",
    "                    max_q_value_list.append(max_q_value)\n",
    "                    # 记录 Q 值\n",
    "\n",
    "                    # 选择并执行动作\n",
    "                    action = agent.take_action(state)\n",
    "                    # 使用 epsilon-greedy 策略选择动作\n",
    "                    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                    # 执行动作，获取结果\n",
    "\n",
    "                    # 更新回放缓冲区\n",
    "                    replay_buffer.add(state, action, reward, next_state, done=terminated or truncated)\n",
    "                    # 存储新经验\n",
    "\n",
    "                    # 训练 Q 网络\n",
    "                    assert replay_buffer.size() > minimal_size\n",
    "                    # 确保缓冲区足够大\n",
    "                    b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                    # 采样 128 条经验\n",
    "                    transition_dict = {\n",
    "                        'states': b_s,\n",
    "                        'actions': b_a,\n",
    "                        'next_states': b_ns,\n",
    "                        'rewards': b_r,\n",
    "                        'dones': b_d\n",
    "                    }\n",
    "                    # 组织批量数据\n",
    "                    agent.update(transition_dict)\n",
    "                    # 更新 Q 网络，计算 Q 学习损失并优化\n",
    "\n",
    "                    state = next_state\n",
    "                    # 更新状态\n",
    "                    episode_return += reward\n",
    "                    # 累加回报\n",
    "\n",
    "                    if terminated or truncated:\n",
    "                        env.render()\n",
    "                        # 回合结束时渲染\n",
    "                        if terminated:\n",
    "                            print(f\"Episode {num_episodes / 20 * i + i_episode + 1}: Goal reached!\")   \n",
    "                        break\n",
    "                    #env.render()\n",
    "                    # （注释掉）可每步渲染，但会减慢训练\n",
    "\n",
    "                return_list.append(episode_return)\n",
    "                # 记录回合回报\n",
    "                if (i_episode + 1) % 10 == 0:\n",
    "                    pbar.set_postfix({\n",
    "                        'episode': '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                        'return': '%.3f' % np.mean(return_list[-10:])\n",
    "                    })\n",
    "                    # 每 10 回合更新进度条，显示回合数和最近 10 次平均回报\n",
    "                pbar.update(1)\n",
    "                # 更新进度条\n",
    "\n",
    "        #env.render()\n",
    "        # （注释掉）迭代结束时可渲染\n",
    "        agent.epsilon += (epsilon_end - epsilon_start) / 10\n",
    "        # 调整 epsilon，注意：应为衰减，可能需改为 agent.epsilon = epsilon_start + (epsilon_end - epsilon_start) * i / 19\n",
    "\n",
    "    # 显示训练结果\n",
    "    mv_return_list = moving_average(return_list, 29)\n",
    "    # 计算 29 步滑动平均回报\n",
    "    episodes_list = list(range(len(return_list)))\n",
    "    # 生成回合索引\n",
    "    plt.figure(figsize=(12,8))\n",
    "    # 创建 12x8 英寸画布\n",
    "    plt.plot(episodes_list, return_list, label='raw', alpha=0.5)\n",
    "    # 绘制原始回报，半透明\n",
    "    plt.plot(episodes_list, mv_return_list, label='moving ave')\n",
    "    # 绘制平滑回报\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Returns')\n",
    "    plt.title(f'{agent._get_name()} on RollingBall')\n",
    "    # 设置标题，包含代理名称（如 DQN）\n",
    "    plt.legend()\n",
    "    # 显示图例\n",
    "    plt.savefig(f'./result/{agent._get_name()}.png')\n",
    "    # 保存回报曲线\n",
    "    plt.show()\n",
    "    # 显示图像\n",
    "    # 显示最大 Q 值\n",
    "    frames_list = list(range(len(max_q_value_list)))\n",
    "    # 生成帧索引\n",
    "    plt.plot(frames_list, max_q_value_list)\n",
    "    # 绘制平滑 Q 值曲线\n",
    "    plt.axhline(max(max_q_value_list), c='orange', ls='--')\n",
    "    # 标记最大 Q 值\n",
    "    plt.xlabel('Frames')\n",
    "    plt.ylabel('Max Q_value')\n",
    "    plt.title(f'{agent._get_name()} on RollingBall')\n",
    "    # 设置标题\n",
    "    plt.savefig(f'./result/{agent._get_name()}_MaxQ.png')\n",
    "    # 保存 Q 值曲线\n",
    "    plt.show()\n",
    "    # 显示图像"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
