{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4252c4d2",
   "metadata": {},
   "source": [
    "RollingBall:玩家（或智能体）通过施加力控制小球，从网格起点（[0.2*宽, 0.2*高]）移动到目标点（[0.8*宽, 0.8*高]）。\n",
    "\n",
    "核心特点：\n",
    "- **环境**：10x10 网格（可调），小球受力、摩擦（系数 0.0046）、边界碰撞（恢复系数 0.8）影响，速度上限 5.0。\n",
    "- **动作**：连续力（[-0.1, 0.1]），可离散化为 5x5 动作（[-0.8, -0.4, 0, 0.4, 0.8]）并展平为 25 个动作。\n",
    "- **奖励**：每步 -2.0，撞墙 -10.0，到达目标 +300.0。\n",
    "- **渲染**：Pygame 显示蓝色小球、紫色目标，可选灰色轨迹，300x300 像素窗口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93377cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from gym.utils.env_checker import check_env\n",
    "from RL_DQN_Class import DQN,ReplayBuffer\n",
    "from gym.wrappers import TimeLimit\n",
    "from two_dimensional_rolling_motion import RollingBall, DiscreteActionWrapper, FlattenActionSpaceWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381e5f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0:   8%|▊         | 4/50 [00:01<00:12,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4.0: Goal reached!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0:  12%|█▏        | 6/50 [00:01<00:09,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6.0: Goal reached!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0:  28%|██▊       | 14/50 [00:03<00:10,  3.60it/s, episode=10, return=-175.400]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 139\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m replay_buffer\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m>\u001b[39m minimal_size\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# 确保缓冲区足够大\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m b_s, b_a, b_r, b_ns, b_d \u001b[38;5;241m=\u001b[39m \u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# 采样 128 条经验\u001b[39;00m\n\u001b[0;32m    141\u001b[0m transition_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m'\u001b[39m: b_s,\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m'\u001b[39m: b_a,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdones\u001b[39m\u001b[38;5;124m'\u001b[39m: b_d\n\u001b[0;32m    147\u001b[0m }\n",
      "File \u001b[1;32mf:\\Learning_project\\RL_Learning\\RL_Learning_Practice\\two_dimensional_rolling_motion_Problem\\RL_DQN_Class.py:21\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     19\u001b[0m transitions \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer, batch_size)\n\u001b[0;32m     20\u001b[0m state, action, reward, next_state, done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mtransitions)\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(state), np\u001b[38;5;241m.\u001b[39marray(action), reward, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m, done\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    def moving_average(a, window_size):\n",
    "        ''' 生成序列 a 的滑动平均序列 '''\n",
    "        # 计算滑动平均，平滑序列（如回合回报），便于绘图时观察趋势\n",
    "        # 参数：a（输入序列，如回报列表），window_size（窗口大小，后续设为 29）\n",
    "        cumulative_sum = np.cumsum(np.insert(a, 0, 0))\n",
    "        # 计算累积和，插入 0 便于差分计算窗口平均值\n",
    "        middle = (cumulative_sum[window_size:] - cumulative_sum[:-window_size]) / window_size\n",
    "        # 中间部分：通过累积和差分计算窗口平均值\n",
    "        r = np.arange(1, window_size-1, 2)\n",
    "        # 生成奇数序列 [1, 3, 5, ...]，用于处理开头和结尾的小窗口\n",
    "        begin = np.cumsum(a[:window_size-1])[::2] / r\n",
    "        # 开头部分：计算前 window_size-1 个元素的小窗口平均值\n",
    "        end = (np.cumsum(a[:-window_size:-1])[::2] / r)[::-1]\n",
    "        # 结尾部分：计算最后 window_size-1 个元素的反向小窗口平均值，翻转对齐\n",
    "        return np.concatenate((begin, middle, end))\n",
    "        # 拼接开头、中间、结尾，返回等长滑动平均序列\n",
    "\n",
    "    def set_seed(env, seed=42):\n",
    "        ''' 设置随机种子 '''\n",
    "        # 设置随机种子，确保环境和算法行为一致，实验可重复\n",
    "        env.action_space.seed(seed)\n",
    "        # 为动作空间设置种子，确保动作采样一致\n",
    "        env.reset(seed=seed)\n",
    "        # 重置环境，应用种子，确保初始状态一致\n",
    "        random.seed(seed)\n",
    "        # 设置 Python random 模块种子，用于随机动作选择\n",
    "        np.random.seed(seed)\n",
    "        # 设置 NumPy 随机种子，用于数组操作和采样\n",
    "        torch.manual_seed(seed)\n",
    "        # 设置 PyTorch 种子，确保神经网络初始化和训练一致\n",
    "\n",
    "    state_dim = 4                               # 环境观测维度\n",
    "    # 状态维度为 4，对应 RollingBall 环境的观测 [x_position, y_position, x_velocity, y_velocity]\n",
    "    action_dim = 1                              # 环境动作维度\n",
    "    action_bins = 10                            # 动作离散 bins 数量\n",
    "    action_range = action_bins * action_bins    # 环境动作空间大小\n",
    "    hidden_dim = 128                             # 神经网络隐藏层大小\n",
    "    # 学习率不建议太大，很难收敛\n",
    "    lr = 0.001                                   # 学习率\n",
    "    num_episodes = 1000                         # 总训练回合数\n",
    "    gamma = 0.99                                # 折扣因子\n",
    "    epsilon_start = 0.1                        # 初始探索率\n",
    "    epsilon_end = 0.05                        # 最终探索率\n",
    "    tau = 1000                        # 目标网络更新参数\n",
    "    buffer_size = 10000                         # 回放缓冲区容量\n",
    "    minimal_size = 5000                         # 训练前最小缓冲区大小\n",
    "    batch_size = 128                            # 批量大小\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    # 优先使用 GPU（CUDA），否则使用 CPU，加速神经网络计算\n",
    "\n",
    "    # 构建环境\n",
    "    env = RollingBall(width=5, height=5, show_epi=True)\n",
    "    # 创建 5x5 的 RollingBall 环境，显示小球轨迹\n",
    "    # 参考 GridWorld.py：\n",
    "    # - 初始位置：(1, 1)（0.2 * 5）\n",
    "    # - 目标位置：(4, 4)（0.8 * 5）\n",
    "    # - 奖励：每步 -2.0，撞墙 -10.0，目标 +300.0\n",
    "    # - 动作空间：Box([-0.1, 0.1], shape=(2,))，x 和 y 方向的力\n",
    "    # - 观测空间：Box([0, 0, -5, -5], [5, 5, 5, 5])，[x, y, vx, vy]\n",
    "    env = FlattenActionSpaceWrapper(DiscreteActionWrapper(env, bins=10))\n",
    "    # 包装动作空间：\n",
    "    # 1. DiscreteActionWrapper（参考 Hashposotion.py）：\n",
    "    #    - 将连续动作离散化为 MultiDiscrete([10, 10])\n",
    "    #    - 每个维度 10 个 bins，映射到 [-1, 1]，步长 0.2\n",
    "    # 2. FlattenActionSpaceWrapper：\n",
    "    #    - 展平为 Discrete(100)，动作索引 0-99\n",
    "    env = TimeLimit(env, 100)\n",
    "    # 限制每回合最大 100 步，超限返回 truncated=True\n",
    "    check_env(env.unwrapped)\n",
    "    # 检查原始 RollingBall 环境是否符合 Gym 规范\n",
    "    set_seed(env, seed=42)\n",
    "    # 设置环境随机种子，确保一致性\n",
    "\n",
    "    # 构建代理和回放缓冲区\n",
    "    replay_buffer = ReplayBuffer(buffer_size)\n",
    "    # 创建回放缓冲区，容量 10000，存储经验 (state, action, reward, next_state, done)\n",
    "    agent = DQN(state_dim, hidden_dim, action_dim, action_range, lr, gamma, epsilon_start, tau, device)\n",
    "    # 创建 DQN 代理（假设 DQN 类已定义）\n",
    "    # 配置：4 维状态输入，100 个动作输出，32 单元隐藏层，学习率 0.001，折扣因子 0.99\n",
    "    # 功能：epsilon-greedy 动作选择，Q 网络更新，目标网络定期同步\n",
    "\n",
    "    # 随机动作填充回放缓冲区\n",
    "    state, _ = env.reset()\n",
    "    # 重置环境，获取初始状态\n",
    "    while replay_buffer.size() <= minimal_size:\n",
    "        action = env.action_space.sample()\n",
    "        # 随机选择动作（0-99）\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        # 执行动作，获取下一状态、奖励、终止标志等\n",
    "        replay_buffer.add(state, action, reward, next_state, done=terminated or truncated)\n",
    "        # 存储经验，done 表示回合是否结束\n",
    "        state = next_state\n",
    "        # 更新状态\n",
    "        if terminated or truncated:\n",
    "            env.render()\n",
    "            # 回合结束时渲染，显示小球轨迹\n",
    "            state, _ = env.reset()\n",
    "            # 重置环境\n",
    "        #print(replay_buffer.size())\n",
    "        # （注释掉）可打印缓冲区大小，监控填充进度\n",
    "\n",
    "    # 开始训练\n",
    "    return_list = []\n",
    "    # 存储每回合总回报\n",
    "    max_q_value_list = []\n",
    "    # 存储平滑的最大 Q 值\n",
    "    max_q_value = 0\n",
    "    # 用于平滑 Q 值的初始值\n",
    "    for i in range(20):\n",
    "        # 分 20 次迭代，每迭代 50 回合，共 1000 回合\n",
    "        with tqdm(total=int(num_episodes / 20), desc='Iteration %d' % i) as pbar:\n",
    "            # 使用 tqdm 显示进度条\n",
    "            for i_episode in range(int(num_episodes / 20)):\n",
    "                # 每迭代 50 回合\n",
    "                episode_return = 0\n",
    "                # 初始化回合回报\n",
    "                state, _ = env.reset()\n",
    "                # 重置环境\n",
    "                while True:\n",
    "                    # 计算并平滑最大 Q 值\n",
    "                    max_q_value = agent.max_q_value_of_given_state(state) * 0.005 + max_q_value * 0.995\n",
    "                    # 使用指数平滑，权重 0.005（新值）和 0.995（旧值）\n",
    "                    max_q_value_list.append(max_q_value)\n",
    "                    # 记录 Q 值\n",
    "\n",
    "                    # 选择并执行动作\n",
    "                    action = agent.take_action(state)\n",
    "                    # 使用 epsilon-greedy 策略选择动作\n",
    "                    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                    # 执行动作，获取结果\n",
    "\n",
    "                    # 更新回放缓冲区\n",
    "                    replay_buffer.add(state, action, reward, next_state, done=terminated or truncated)\n",
    "                    # 存储新经验\n",
    "\n",
    "                    # 训练 Q 网络\n",
    "                    assert replay_buffer.size() > minimal_size\n",
    "                    # 确保缓冲区足够大\n",
    "                    b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                    # 采样 128 条经验\n",
    "                    transition_dict = {\n",
    "                        'states': b_s,\n",
    "                        'actions': b_a,\n",
    "                        'next_states': b_ns,\n",
    "                        'rewards': b_r,\n",
    "                        'dones': b_d\n",
    "                    }\n",
    "                    # 组织批量数据\n",
    "                    agent.update(transition_dict)\n",
    "                    # 更新 Q 网络，计算 Q 学习损失并优化\n",
    "\n",
    "                    state = next_state\n",
    "                    # 更新状态\n",
    "                    episode_return += reward\n",
    "                    # 累加回报\n",
    "\n",
    "                    if terminated or truncated:\n",
    "                        env.render()\n",
    "                        # 回合结束时渲染\n",
    "                        if terminated:\n",
    "                            print(f\"Episode {num_episodes / 20 * i + i_episode + 1}: Goal reached!\")   \n",
    "                        break\n",
    "                    #env.render()\n",
    "                    # （注释掉）可每步渲染，但会减慢训练\n",
    "\n",
    "                return_list.append(episode_return)\n",
    "                # 记录回合回报\n",
    "                if (i_episode + 1) % 10 == 0:\n",
    "                    pbar.set_postfix({\n",
    "                        'episode': '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                        'return': '%.3f' % np.mean(return_list[-10:])\n",
    "                    })\n",
    "                    # 每 10 回合更新进度条，显示回合数和最近 10 次平均回报\n",
    "                pbar.update(1)\n",
    "                # 更新进度条\n",
    "\n",
    "        #env.render()\n",
    "        # （注释掉）迭代结束时可渲染\n",
    "        agent.epsilon += (epsilon_end - epsilon_start) / 10\n",
    "        # 调整 epsilon，注意：应为衰减，可能需改为 agent.epsilon = epsilon_start + (epsilon_end - epsilon_start) * i / 19\n",
    "\n",
    "    # 显示训练结果\n",
    "    mv_return_list = moving_average(return_list, 29)\n",
    "    # 计算 29 步滑动平均回报\n",
    "    episodes_list = list(range(len(return_list)))\n",
    "    # 生成回合索引\n",
    "    plt.figure(figsize=(12,8))\n",
    "    # 创建 12x8 英寸画布\n",
    "    plt.plot(episodes_list, return_list, label='raw', alpha=0.5)\n",
    "    # 绘制原始回报，半透明\n",
    "    plt.plot(episodes_list, mv_return_list, label='moving ave')\n",
    "    # 绘制平滑回报\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Returns')\n",
    "    plt.title(f'{agent._get_name()} on RollingBall')\n",
    "    # 设置标题，包含代理名称（如 DQN）\n",
    "    plt.legend()\n",
    "    # 显示图例\n",
    "    plt.savefig(f'./result/{agent._get_name()}.png')\n",
    "    # 保存回报曲线\n",
    "    plt.show()\n",
    "    # 显示图像\n",
    "    # 显示最大 Q 值\n",
    "    frames_list = list(range(len(max_q_value_list)))\n",
    "    # 生成帧索引\n",
    "    plt.plot(frames_list, max_q_value_list)\n",
    "    # 绘制平滑 Q 值曲线\n",
    "    plt.axhline(max(max_q_value_list), c='orange', ls='--')\n",
    "    # 标记最大 Q 值\n",
    "    plt.xlabel('Frames')\n",
    "    plt.ylabel('Max Q_value')\n",
    "    plt.title(f'{agent._get_name()} on RollingBall')\n",
    "    # 设置标题\n",
    "    plt.savefig(f'./result/{agent._get_name()}_MaxQ.png')\n",
    "    # 保存 Q 值曲线\n",
    "    plt.show()\n",
    "    # 显示图像"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
